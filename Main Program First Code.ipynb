{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "150e05fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image counts per class:\n",
      "Clean: 1493\n",
      "Dusty: 1069\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = './Detect_solar_dust'\n",
    "\n",
    "# Define class folders\n",
    "classes = ['Clean', 'Dusty']\n",
    "\n",
    "# Count images\n",
    "image_counts = {}\n",
    "for cls in classes:\n",
    "    class_path = os.path.join(dataset_path, cls)\n",
    "    image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "    image_counts[cls] = len(image_files)\n",
    "\n",
    "# Print counts\n",
    "print(\"Image counts per class:\")\n",
    "for cls, count in image_counts.items():\n",
    "    print(f\"{cls}: {count}\")\n",
    "\n",
    "# Assign colors (one per class)\n",
    "colors = ['#1f77b4', '#ff7f0e']  # Clean → blue, Dusty → orange\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(image_counts.keys(), image_counts.values(), color=colors)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Number of Images per Class')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a9af157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected image counts per class:\n",
      "Clean: 150\n",
      "Dusty: 30\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = './Detect_solar_dust'\n",
    "\n",
    "# Define class folders\n",
    "classes = ['Clean', 'Dusty']\n",
    "\n",
    "# Initialize image count dictionary\n",
    "image_counts = {}\n",
    "\n",
    "for cls in classes:\n",
    "    class_path = os.path.join(dataset_path, cls)\n",
    "    image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "    \n",
    "    if cls == 'Dusty':\n",
    "        # Take 30 images from Dusty\n",
    "        selected_files = image_files[:30]\n",
    "        image_counts[cls] = len(selected_files)\n",
    "    else:\n",
    "        # Take 150 images from Clean\n",
    "        selected_files = image_files[:150]\n",
    "        image_counts[cls] = len(selected_files)\n",
    "\n",
    "# Print counts\n",
    "print(\"Selected image counts per class:\")\n",
    "for cls, count in image_counts.items():\n",
    "    print(f\"{cls}: {count}\")\n",
    "\n",
    "# Assign colors (unique for each class)\n",
    "colors = ['#1f77b4', '#ff7f0e']  # Clean → blue, Dusty → orange\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(image_counts.keys(), image_counts.values(), color=colors)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Number of Images per Class')\n",
    "#plt.savefig('image_counts_per_class.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ceb074df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final image counts per class:\n",
      "Clean: 150\n",
      "Dusty: 90\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the dataset paths\n",
    "dataset_path = './Detect_solar_dust'\n",
    "generated_path = r'D:\\Raj Kumar\\Hybrid Augmentation Approach\\generated_solar_faults'\n",
    "\n",
    "# Define class folders\n",
    "classes = ['Clean', 'Dusty']\n",
    "\n",
    "# Initialize image count dictionary\n",
    "image_counts = {}\n",
    "\n",
    "# Process Clean and Dusty classes\n",
    "for cls in classes:\n",
    "    class_path = os.path.join(dataset_path, cls)\n",
    "    image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "    \n",
    "    if cls == 'Dusty':\n",
    "        # Take 30 original Dusty images\n",
    "        selected_dusty_files = image_files[:30]\n",
    "                \n",
    "        # Count 60 generated images\n",
    "        generated_files = [f for f in os.listdir(generated_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "        selected_generated_files = generated_files[:60]\n",
    "        \n",
    "        total_dusty = len(selected_dusty_files) + len(selected_generated_files)\n",
    "        image_counts[cls] = total_dusty\n",
    "    else:\n",
    "        # Take 150 Clean images\n",
    "        selected_clean_files = image_files[:150]\n",
    "        image_counts[cls] = len(selected_clean_files)\n",
    "\n",
    "# Print counts\n",
    "print(\"Final image counts per class:\")\n",
    "for cls, count in image_counts.items():\n",
    "    print(f\"{cls}: {count}\")\n",
    "    \n",
    "# Assign colors (unique for each class)\n",
    "colors = ['#1f77b4', '#ff7f0e']  # Clean → blue, Dusty → orange\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(image_counts.keys(), image_counts.values(), color=colors)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Number of Images per Class (Dusty: 30 Original + 60 Generated)')\n",
    "#plt.savefig('image_counts_per_class.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e865c",
   "metadata": {},
   "source": [
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "097e6185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features (Clean): 100%|██████████| 150/150 [00:19<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to CLEAN_FEATURES.npy with shape: (150, 1024)\n",
      "✅ Saved to CLEAN_FEATURES.csv with shape: (150, 1026)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features (Dusty): 100%|██████████| 90/90 [00:07<00:00, 11.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to DUSTY_FEATURES.npy with shape: (90, 1024)\n",
      "✅ Saved to DUSTY_FEATURES.csv with shape: (90, 1026)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "clean_path = './Detect_solar_dust/Clean'\n",
    "dusty_path = './Detect_solar_dust/Dusty'\n",
    "generated_path = r'D:\\Raj Kumar\\Hybrid Augmentation Approach\\generated_solar_faults'\n",
    "\n",
    "# Collect selected image paths\n",
    "# 150 Clean images\n",
    "clean_files = [os.path.join(clean_path, f) for f in os.listdir(clean_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))][:150]\n",
    "# 30 original Dusty images\n",
    "dusty_files = [os.path.join(dusty_path, f) for f in os.listdir(dusty_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))][:30]\n",
    "# 60 generated images\n",
    "generated_files = [os.path.join(generated_path, f) for f in os.listdir(generated_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))][:60]\n",
    "\n",
    "# Combine Dusty and Generated files for Dusty category\n",
    "dusty_all_files = dusty_files + generated_files\n",
    "\n",
    "# Load pretrained DenseNet121\n",
    "model = models.densenet121(weights='IMAGENET1K_V1')\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Remove classifier, keep only feature extractor\n",
    "feature_extractor = model.features\n",
    "\n",
    "# Define transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Feature extraction function\n",
    "def extract_features(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = feature_extractor(img)\n",
    "        features = torch.nn.functional.adaptive_avg_pool2d(features, (1, 1))\n",
    "        features = features.view(features.size(0), -1)  # Flatten to (1, 1024)\n",
    "    return features.cpu().numpy()\n",
    "\n",
    "# Extract features for Clean images\n",
    "clean_features_list = []\n",
    "clean_file_names = []\n",
    "for img_path in tqdm(clean_files, desc='Extracting features (Clean)'):\n",
    "    features = extract_features(img_path)\n",
    "    clean_features_list.append(features[0])\n",
    "    clean_file_names.append(os.path.basename(img_path))\n",
    "\n",
    "# Convert Clean features to numpy array → shape (150, 1024)\n",
    "clean_features_array = np.array(clean_features_list)\n",
    "\n",
    "# Save Clean features directly in current directory\n",
    "np.save('CLEAN_FEATURES.npy', clean_features_array)\n",
    "print(\"✅ Saved to CLEAN_FEATURES.npy with shape:\", clean_features_array.shape)\n",
    "\n",
    "# Save Clean features to .csv\n",
    "clean_df = pd.DataFrame(clean_features_array)\n",
    "clean_df.insert(0, 'filename', clean_file_names)\n",
    "clean_df.insert(1, 'label', [0] * len(clean_files))  # Label: 0 for Clean\n",
    "clean_df.to_csv('CLEAN_FEATURES.csv', index=False)\n",
    "print(\"✅ Saved to CLEAN_FEATURES.csv with shape:\", clean_df.shape)\n",
    "\n",
    "# Extract features for Dusty images (original + generated)\n",
    "dusty_features_list = []\n",
    "dusty_file_names = []\n",
    "for img_path in tqdm(dusty_all_files, desc='Extracting features (Dusty)'):\n",
    "    features = extract_features(img_path)\n",
    "    dusty_features_list.append(features[0])\n",
    "    dusty_file_names.append(os.path.basename(img_path))\n",
    "\n",
    "# Convert Dusty features to numpy array → shape (90, 1024)\n",
    "dusty_features_array = np.array(dusty_features_list)\n",
    "\n",
    "# Save Dusty features directly in current directory\n",
    "np.save('DUSTY_FEATURES.npy', dusty_features_array)\n",
    "print(\"✅ Saved to DUSTY_FEATURES.npy with shape:\", dusty_features_array.shape)\n",
    "\n",
    "# Save Dusty features to .csv\n",
    "dusty_df = pd.DataFrame(dusty_features_array)\n",
    "dusty_df.insert(0, 'filename', dusty_file_names)\n",
    "dusty_df.insert(1, 'label', [1] * len(dusty_all_files))  # Label: 1 for Dusty/Generated\n",
    "dusty_df.to_csv('DUSTY_FEATURES.csv', index=False)\n",
    "print(\"✅ Saved to DUSTY_FEATURES.csv with shape:\", dusty_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6051af32",
   "metadata": {},
   "source": [
    "#### Applying SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8983cbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE → Dusty: 90, Clean: 150\n",
      "✅ SMOTE completed.\n",
      "Final Dusty samples: 150\n",
      "Final Clean samples: 150\n",
      "Synthetic Dusty samples generated: 60\n",
      "Synthetic Clean samples generated: 0\n",
      "Final dataset shape: (300, 1025)\n",
      "✅ Saved full balanced dataset as X_balanced.npy, y_balanced.npy, X_balanced.csv, y_balanced.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load feature files from CSV (skip 'filename' column)\n",
    "dusty_features = pd.read_csv('DUSTY_FEATURES.csv').drop(columns=['filename']).values\n",
    "clean_features = pd.read_csv('CLEAN_FEATURES.csv').drop(columns=['filename']).values\n",
    "\n",
    "# Combine features and create labels\n",
    "X = np.vstack([dusty_features, clean_features])\n",
    "y = np.array([0] * len(dusty_features) + [1] * len(clean_features))  # 0 = Dusty, 1 = Clean\n",
    "\n",
    "print(f\"Before SMOTE → Dusty: {sum(y==0)}, Clean: {sum(y==1)}\")\n",
    "\n",
    "# Determine target count to balance both classes\n",
    "target_count = max(sum(y == 0), sum(y == 1))\n",
    "\n",
    "# Apply SMOTE to balance both classes\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Calculate how many synthetic samples were added\n",
    "num_dusty_generated = sum(y_resampled == 0) - sum(y == 0)\n",
    "num_clean_generated = sum(y_resampled == 1) - sum(y == 1)\n",
    "\n",
    "# Save synthetic samples separately\n",
    "dusty_indices = np.where(y_resampled == 0)[0]\n",
    "clean_indices = np.where(y_resampled == 1)[0]\n",
    "\n",
    "synthetic_dusty = X_resampled[dusty_indices][len(dusty_features):]\n",
    "synthetic_clean = X_resampled[clean_indices][len(clean_features):]\n",
    "\n",
    "np.save('SYNTHETIC_DUSTY_FEATURES.npy', synthetic_dusty)\n",
    "np.save('SYNTHETIC_CLEAN_FEATURES.npy', synthetic_clean)\n",
    "\n",
    "# Final summary\n",
    "final_dusty = sum(y_resampled == 0)\n",
    "final_clean = sum(y_resampled == 1)\n",
    "\n",
    "print(\"✅ SMOTE completed.\")\n",
    "print(\"Final Dusty samples:\", final_dusty)\n",
    "print(\"Final Clean samples:\", final_clean)\n",
    "print(\"Synthetic Dusty samples generated:\", num_dusty_generated)\n",
    "print(\"Synthetic Clean samples generated:\", num_clean_generated)\n",
    "print(\"Final dataset shape:\", X_resampled.shape)\n",
    "\n",
    "# Save full balanced dataset\n",
    "np.save('X_balanced.npy', X_resampled)\n",
    "np.save('y_balanced.npy', y_resampled)\n",
    "pd.DataFrame(X_resampled).to_csv('X_balanced.csv', index=False)\n",
    "pd.DataFrame(y_resampled, columns=['label']).to_csv('y_balanced.csv', index=False)\n",
    "print(\"✅ Saved full balanced dataset as X_balanced.npy, y_balanced.npy, X_balanced.csv, y_balanced.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e471fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dusty samples: 150\n",
      "✅ Clean samples: 150\n",
      "✅ Dataset is perfectly balanced! 💥\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load balanced labels\n",
    "y_resampled = np.load('y_balanced.npy')\n",
    "\n",
    "# Check counts\n",
    "dusty_count = sum(y_resampled == 0)\n",
    "clean_count = sum(y_resampled == 1)\n",
    "\n",
    "print(f\"✅ Dusty samples: {dusty_count}\")\n",
    "print(f\"✅ Clean samples: {clean_count}\")\n",
    "\n",
    "# Check if balanced\n",
    "if dusty_count == clean_count:\n",
    "    print(\"✅ Dataset is perfectly balanced! 💥\")\n",
    "else:\n",
    "    print(\"⚠️ Dataset is imbalanced.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd4dc86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Actual, generated, SMOTE counts (set these manually or calculate)\n",
    "actual_dusty = 30\n",
    "generated_dusty = 60\n",
    "smote_dusty = 60\n",
    "\n",
    "# Final balanced dataset (load from y_balanced.npy if needed)\n",
    "final_dusty = actual_dusty + generated_dusty + smote_dusty  # or get from np.sum(y_resampled == 0)\n",
    "final_clean = final_dusty  # since you balanced to match\n",
    "\n",
    "# Plot 1: Breakdown of Dusty samples\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(['Actual Dusty', 'Generated (SD)', 'SMOTE'], [actual_dusty, generated_dusty, smote_dusty], color=['blue', 'orange', 'green'])\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Dusty Class Breakdown')\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Final Clean vs. Dusty balance\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.bar(['Dusty (Final)', 'Clean (Final)'], [final_dusty, final_clean], color=['purple', 'red'])\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Final Clean vs. Dusty (Balanced with SMOTE + SD)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3e1dfc",
   "metadata": {},
   "source": [
    "#### Reshaping and Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afbd5ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved resampled features and labels to disk.\n"
     ]
    }
   ],
   "source": [
    "# After SMOTE:\n",
    "# X_resampled → features, shape (n_samples, 1024)\n",
    "# y_resampled → labels, shape (n_samples, )\n",
    "\n",
    "np.save('X_resampled.npy', X_resampled)\n",
    "np.save('y_resampled.npy', y_resampled)\n",
    "\n",
    "print(\"Saved resampled features and labels to disk.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4163839f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final class counts after SMOTE:\n",
      "Dusty: 150\n",
      "Clean: 150\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the saved arrays\n",
    "X_resampled = np.load('X_resampled.npy')\n",
    "y_resampled = np.load('y_resampled.npy')\n",
    "\n",
    "# Count samples per class\n",
    "unique, counts = np.unique(y_resampled, return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "\n",
    "# Print class counts\n",
    "print(\"Final class counts after SMOTE:\")\n",
    "for cls, count in class_counts.items():\n",
    "    label = 'Dusty' if cls == 0 else 'Clean'\n",
    "    print(f\"{label}: {count}\")\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(['Dusty', 'Clean'], [class_counts.get(0, 0), class_counts.get(1, 0)], color=['#ff7f0e', '#1f77b4'])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Dataset Balance After SMOTE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f947904",
   "metadata": {},
   "source": [
    "#### Splitting Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d5688f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a51f7e",
   "metadata": {},
   "source": [
    "### TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33b18cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n",
      "Running 5-fold cross-validation with TabNet...\n",
      "\n",
      "=== Fold 1 (TabNet) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\kinzaenv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.21768 | train_accuracy: 0.48333 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 1.41429 | train_accuracy: 0.53333 | val_accuracy: 0.61667 |  0:00:00s\n",
      "epoch 2  | loss: 1.05707 | train_accuracy: 0.55417 | val_accuracy: 0.45    |  0:00:01s\n",
      "epoch 3  | loss: 1.18987 | train_accuracy: 0.5125  | val_accuracy: 0.48333 |  0:00:01s\n",
      "epoch 4  | loss: 1.03583 | train_accuracy: 0.52917 | val_accuracy: 0.46667 |  0:00:01s\n",
      "epoch 5  | loss: 0.68112 | train_accuracy: 0.59583 | val_accuracy: 0.58333 |  0:00:01s\n",
      "epoch 6  | loss: 0.77058 | train_accuracy: 0.65417 | val_accuracy: 0.56667 |  0:00:01s\n",
      "epoch 7  | loss: 0.65074 | train_accuracy: 0.67917 | val_accuracy: 0.53333 |  0:00:02s\n",
      "epoch 8  | loss: 0.64795 | train_accuracy: 0.6625  | val_accuracy: 0.58333 |  0:00:02s\n",
      "epoch 9  | loss: 0.63716 | train_accuracy: 0.7125  | val_accuracy: 0.6     |  0:00:02s\n",
      "epoch 10 | loss: 0.57329 | train_accuracy: 0.7375  | val_accuracy: 0.6     |  0:00:02s\n",
      "epoch 11 | loss: 0.56229 | train_accuracy: 0.73333 | val_accuracy: 0.63333 |  0:00:02s\n",
      "epoch 12 | loss: 0.53492 | train_accuracy: 0.72083 | val_accuracy: 0.58333 |  0:00:03s\n",
      "epoch 13 | loss: 0.57898 | train_accuracy: 0.70833 | val_accuracy: 0.61667 |  0:00:03s\n",
      "epoch 14 | loss: 0.41621 | train_accuracy: 0.71667 | val_accuracy: 0.65    |  0:00:03s\n",
      "epoch 15 | loss: 0.38085 | train_accuracy: 0.74167 | val_accuracy: 0.68333 |  0:00:03s\n",
      "epoch 16 | loss: 0.33337 | train_accuracy: 0.77083 | val_accuracy: 0.66667 |  0:00:04s\n",
      "epoch 17 | loss: 0.35233 | train_accuracy: 0.80833 | val_accuracy: 0.68333 |  0:00:04s\n",
      "epoch 18 | loss: 0.37032 | train_accuracy: 0.8625  | val_accuracy: 0.73333 |  0:00:04s\n",
      "epoch 19 | loss: 0.33885 | train_accuracy: 0.87083 | val_accuracy: 0.78333 |  0:00:04s\n",
      "epoch 20 | loss: 0.33019 | train_accuracy: 0.86667 | val_accuracy: 0.71667 |  0:00:04s\n",
      "epoch 21 | loss: 0.29238 | train_accuracy: 0.875   | val_accuracy: 0.71667 |  0:00:05s\n",
      "epoch 22 | loss: 0.26393 | train_accuracy: 0.87917 | val_accuracy: 0.73333 |  0:00:05s\n",
      "epoch 23 | loss: 0.308   | train_accuracy: 0.88333 | val_accuracy: 0.75    |  0:00:05s\n",
      "epoch 24 | loss: 0.21042 | train_accuracy: 0.9125  | val_accuracy: 0.8     |  0:00:05s\n",
      "epoch 25 | loss: 0.26862 | train_accuracy: 0.90833 | val_accuracy: 0.81667 |  0:00:05s\n",
      "epoch 26 | loss: 0.20201 | train_accuracy: 0.925   | val_accuracy: 0.81667 |  0:00:06s\n",
      "epoch 27 | loss: 0.20931 | train_accuracy: 0.94167 | val_accuracy: 0.8     |  0:00:06s\n",
      "epoch 28 | loss: 0.23432 | train_accuracy: 0.94583 | val_accuracy: 0.83333 |  0:00:06s\n",
      "epoch 29 | loss: 0.1566  | train_accuracy: 0.96667 | val_accuracy: 0.8     |  0:00:06s\n",
      "epoch 30 | loss: 0.08122 | train_accuracy: 0.97083 | val_accuracy: 0.85    |  0:00:07s\n",
      "epoch 31 | loss: 0.18425 | train_accuracy: 0.975   | val_accuracy: 0.88333 |  0:00:07s\n",
      "epoch 32 | loss: 0.17856 | train_accuracy: 0.975   | val_accuracy: 0.86667 |  0:00:07s\n",
      "epoch 33 | loss: 0.09803 | train_accuracy: 0.975   | val_accuracy: 0.85    |  0:00:08s\n",
      "epoch 34 | loss: 0.10231 | train_accuracy: 0.97917 | val_accuracy: 0.85    |  0:00:08s\n",
      "epoch 35 | loss: 0.0857  | train_accuracy: 0.98333 | val_accuracy: 0.81667 |  0:00:08s\n",
      "epoch 36 | loss: 0.05516 | train_accuracy: 0.9875  | val_accuracy: 0.8     |  0:00:08s\n",
      "epoch 37 | loss: 0.08957 | train_accuracy: 0.9875  | val_accuracy: 0.83333 |  0:00:08s\n",
      "epoch 38 | loss: 0.1502  | train_accuracy: 0.98333 | val_accuracy: 0.83333 |  0:00:09s\n",
      "epoch 39 | loss: 0.05831 | train_accuracy: 0.98333 | val_accuracy: 0.85    |  0:00:09s\n",
      "epoch 40 | loss: 0.05035 | train_accuracy: 0.98333 | val_accuracy: 0.85    |  0:00:09s\n",
      "epoch 41 | loss: 0.08556 | train_accuracy: 0.98333 | val_accuracy: 0.85    |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 31 and best_val_accuracy = 0.88333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\kinzaenv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Best Validation Accuracy: 0.8833\n",
      "TabNet Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.96      0.80      0.87        30\n",
      "       Dirty       0.83      0.97      0.89        30\n",
      "\n",
      "    accuracy                           0.88        60\n",
      "   macro avg       0.89      0.88      0.88        60\n",
      "weighted avg       0.89      0.88      0.88        60\n",
      "\n",
      "TabNet Confusion Matrix:\n",
      "[[24  6]\n",
      " [ 1 29]]\n",
      "\n",
      "=== Fold 2 (TabNet) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\kinzaenv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.26802 | train_accuracy: 0.49583 | val_accuracy: 0.4     |  0:00:00s\n",
      "epoch 1  | loss: 1.63978 | train_accuracy: 0.55833 | val_accuracy: 0.5     |  0:00:00s\n",
      "epoch 2  | loss: 0.97015 | train_accuracy: 0.4875  | val_accuracy: 0.53333 |  0:00:00s\n",
      "epoch 3  | loss: 1.16573 | train_accuracy: 0.5     | val_accuracy: 0.51667 |  0:00:00s\n",
      "epoch 4  | loss: 1.07574 | train_accuracy: 0.58333 | val_accuracy: 0.66667 |  0:00:01s\n",
      "epoch 5  | loss: 0.72916 | train_accuracy: 0.6     | val_accuracy: 0.58333 |  0:00:01s\n",
      "epoch 6  | loss: 0.73539 | train_accuracy: 0.6     | val_accuracy: 0.65    |  0:00:01s\n",
      "epoch 7  | loss: 0.79543 | train_accuracy: 0.60833 | val_accuracy: 0.6     |  0:00:01s\n",
      "epoch 8  | loss: 0.6991  | train_accuracy: 0.62083 | val_accuracy: 0.48333 |  0:00:01s\n",
      "epoch 9  | loss: 0.61492 | train_accuracy: 0.67083 | val_accuracy: 0.58333 |  0:00:02s\n",
      "epoch 10 | loss: 0.62135 | train_accuracy: 0.675   | val_accuracy: 0.61667 |  0:00:02s\n",
      "epoch 11 | loss: 0.61646 | train_accuracy: 0.6625  | val_accuracy: 0.68333 |  0:00:02s\n",
      "epoch 12 | loss: 0.70404 | train_accuracy: 0.71667 | val_accuracy: 0.73333 |  0:00:02s\n",
      "epoch 13 | loss: 0.62364 | train_accuracy: 0.75833 | val_accuracy: 0.68333 |  0:00:02s\n",
      "epoch 14 | loss: 0.54296 | train_accuracy: 0.725   | val_accuracy: 0.55    |  0:00:03s\n",
      "epoch 15 | loss: 0.48406 | train_accuracy: 0.73333 | val_accuracy: 0.61667 |  0:00:03s\n",
      "epoch 16 | loss: 0.50808 | train_accuracy: 0.75833 | val_accuracy: 0.7     |  0:00:03s\n",
      "epoch 17 | loss: 0.55009 | train_accuracy: 0.82083 | val_accuracy: 0.73333 |  0:00:03s\n",
      "epoch 18 | loss: 0.47055 | train_accuracy: 0.7875  | val_accuracy: 0.73333 |  0:00:03s\n",
      "epoch 19 | loss: 0.41196 | train_accuracy: 0.80417 | val_accuracy: 0.81667 |  0:00:04s\n",
      "epoch 20 | loss: 0.4473  | train_accuracy: 0.85    | val_accuracy: 0.83333 |  0:00:04s\n",
      "epoch 21 | loss: 0.44341 | train_accuracy: 0.85417 | val_accuracy: 0.83333 |  0:00:04s\n",
      "epoch 22 | loss: 0.36716 | train_accuracy: 0.89583 | val_accuracy: 0.81667 |  0:00:04s\n",
      "epoch 23 | loss: 0.35843 | train_accuracy: 0.90417 | val_accuracy: 0.85    |  0:00:05s\n",
      "epoch 24 | loss: 0.33683 | train_accuracy: 0.90833 | val_accuracy: 0.85    |  0:00:05s\n",
      "epoch 25 | loss: 0.42959 | train_accuracy: 0.9     | val_accuracy: 0.83333 |  0:00:05s\n",
      "epoch 26 | loss: 0.26503 | train_accuracy: 0.8625  | val_accuracy: 0.75    |  0:00:05s\n",
      "epoch 27 | loss: 0.31476 | train_accuracy: 0.8625  | val_accuracy: 0.73333 |  0:00:05s\n",
      "epoch 28 | loss: 0.30375 | train_accuracy: 0.89583 | val_accuracy: 0.75    |  0:00:06s\n",
      "epoch 29 | loss: 0.25149 | train_accuracy: 0.925   | val_accuracy: 0.83333 |  0:00:06s\n",
      "epoch 30 | loss: 0.27041 | train_accuracy: 0.95    | val_accuracy: 0.85    |  0:00:06s\n",
      "epoch 31 | loss: 0.23899 | train_accuracy: 0.95    | val_accuracy: 0.85    |  0:00:06s\n",
      "epoch 32 | loss: 0.17818 | train_accuracy: 0.94167 | val_accuracy: 0.81667 |  0:00:06s\n",
      "epoch 33 | loss: 0.11903 | train_accuracy: 0.95    | val_accuracy: 0.81667 |  0:00:07s\n",
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_accuracy = 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\kinzaenv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Best Validation Accuracy: 0.8500\n",
      "TabNet Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.86      0.83      0.85        30\n",
      "       Dirty       0.84      0.87      0.85        30\n",
      "\n",
      "    accuracy                           0.85        60\n",
      "   macro avg       0.85      0.85      0.85        60\n",
      "weighted avg       0.85      0.85      0.85        60\n",
      "\n",
      "TabNet Confusion Matrix:\n",
      "[[25  5]\n",
      " [ 4 26]]\n",
      "\n",
      "=== Fold 3 (TabNet) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\kinzaenv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.47189 | train_accuracy: 0.50417 | val_accuracy: 0.51667 |  0:00:00s\n",
      "epoch 1  | loss: 1.44094 | train_accuracy: 0.5375  | val_accuracy: 0.48333 |  0:00:00s\n",
      "epoch 2  | loss: 1.19931 | train_accuracy: 0.4875  | val_accuracy: 0.56667 |  0:00:00s\n",
      "epoch 3  | loss: 1.18483 | train_accuracy: 0.59583 | val_accuracy: 0.58333 |  0:00:00s\n",
      "epoch 4  | loss: 1.09694 | train_accuracy: 0.59167 | val_accuracy: 0.55    |  0:00:01s\n",
      "epoch 5  | loss: 0.9523  | train_accuracy: 0.58333 | val_accuracy: 0.53333 |  0:00:01s\n",
      "epoch 6  | loss: 0.68326 | train_accuracy: 0.59167 | val_accuracy: 0.53333 |  0:00:01s\n",
      "epoch 7  | loss: 0.7549  | train_accuracy: 0.65833 | val_accuracy: 0.53333 |  0:00:01s\n",
      "epoch 8  | loss: 0.63184 | train_accuracy: 0.68333 | val_accuracy: 0.51667 |  0:00:01s\n",
      "epoch 9  | loss: 0.65188 | train_accuracy: 0.68333 | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 10 | loss: 0.76241 | train_accuracy: 0.6875  | val_accuracy: 0.58333 |  0:00:02s\n",
      "epoch 11 | loss: 0.55323 | train_accuracy: 0.70417 | val_accuracy: 0.55    |  0:00:02s\n",
      "epoch 12 | loss: 0.56021 | train_accuracy: 0.7375  | val_accuracy: 0.58333 |  0:00:02s\n",
      "epoch 13 | loss: 0.57308 | train_accuracy: 0.7125  | val_accuracy: 0.61667 |  0:00:02s\n",
      "epoch 14 | loss: 0.54412 | train_accuracy: 0.7625  | val_accuracy: 0.6     |  0:00:02s\n",
      "epoch 15 | loss: 0.53387 | train_accuracy: 0.7625  | val_accuracy: 0.6     |  0:00:03s\n",
      "epoch 16 | loss: 0.45637 | train_accuracy: 0.79167 | val_accuracy: 0.63333 |  0:00:03s\n",
      "epoch 17 | loss: 0.39267 | train_accuracy: 0.85833 | val_accuracy: 0.65    |  0:00:03s\n",
      "epoch 18 | loss: 0.41441 | train_accuracy: 0.86667 | val_accuracy: 0.68333 |  0:00:03s\n",
      "epoch 19 | loss: 0.36169 | train_accuracy: 0.86667 | val_accuracy: 0.7     |  0:00:03s\n",
      "epoch 20 | loss: 0.37106 | train_accuracy: 0.8625  | val_accuracy: 0.73333 |  0:00:04s\n",
      "epoch 21 | loss: 0.30119 | train_accuracy: 0.85    | val_accuracy: 0.7     |  0:00:04s\n",
      "epoch 22 | loss: 0.31835 | train_accuracy: 0.89583 | val_accuracy: 0.76667 |  0:00:04s\n",
      "epoch 23 | loss: 0.2361  | train_accuracy: 0.9125  | val_accuracy: 0.75    |  0:00:04s\n",
      "epoch 24 | loss: 0.20495 | train_accuracy: 0.9     | val_accuracy: 0.75    |  0:00:04s\n",
      "epoch 25 | loss: 0.20477 | train_accuracy: 0.92083 | val_accuracy: 0.75    |  0:00:05s\n",
      "epoch 26 | loss: 0.18773 | train_accuracy: 0.94583 | val_accuracy: 0.78333 |  0:00:05s\n",
      "epoch 27 | loss: 0.14733 | train_accuracy: 0.94583 | val_accuracy: 0.81667 |  0:00:05s\n",
      "epoch 28 | loss: 0.14885 | train_accuracy: 0.95417 | val_accuracy: 0.85    |  0:00:05s\n",
      "epoch 29 | loss: 0.20093 | train_accuracy: 0.96667 | val_accuracy: 0.85    |  0:00:05s\n",
      "epoch 30 | loss: 0.1293  | train_accuracy: 0.975   | val_accuracy: 0.85    |  0:00:06s\n",
      "epoch 31 | loss: 0.15964 | train_accuracy: 0.975   | val_accuracy: 0.85    |  0:00:06s\n",
      "epoch 32 | loss: 0.14053 | train_accuracy: 0.9875  | val_accuracy: 0.9     |  0:00:06s\n",
      "epoch 33 | loss: 0.09264 | train_accuracy: 0.9875  | val_accuracy: 0.91667 |  0:00:06s\n",
      "epoch 34 | loss: 0.08592 | train_accuracy: 0.97917 | val_accuracy: 0.91667 |  0:00:06s\n",
      "epoch 35 | loss: 0.07197 | train_accuracy: 0.99167 | val_accuracy: 0.93333 |  0:00:07s\n",
      "epoch 36 | loss: 0.06044 | train_accuracy: 1.0     | val_accuracy: 0.95    |  0:00:07s\n",
      "epoch 37 | loss: 0.06532 | train_accuracy: 1.0     | val_accuracy: 0.91667 |  0:00:07s\n",
      "epoch 38 | loss: 0.02679 | train_accuracy: 0.99583 | val_accuracy: 0.93333 |  0:00:07s\n",
      "epoch 39 | loss: 0.03772 | train_accuracy: 0.99167 | val_accuracy: 0.93333 |  0:00:07s\n",
      "epoch 40 | loss: 0.04452 | train_accuracy: 0.99167 | val_accuracy: 0.93333 |  0:00:07s\n",
      "epoch 41 | loss: 0.0668  | train_accuracy: 0.99583 | val_accuracy: 0.93333 |  0:00:08s\n",
      "epoch 42 | loss: 0.01583 | train_accuracy: 0.99583 | val_accuracy: 0.93333 |  0:00:08s\n",
      "epoch 43 | loss: 0.01046 | train_accuracy: 0.99583 | val_accuracy: 0.93333 |  0:00:08s\n",
      "epoch 44 | loss: 0.01239 | train_accuracy: 0.99583 | val_accuracy: 0.93333 |  0:00:08s\n",
      "epoch 45 | loss: 0.0075  | train_accuracy: 0.99583 | val_accuracy: 0.93333 |  0:00:08s\n",
      "epoch 46 | loss: 0.05156 | train_accuracy: 0.99583 | val_accuracy: 0.91667 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_accuracy = 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\kinzaenv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Best Validation Accuracy: 0.9500\n",
      "TabNet Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.97      0.93      0.95        30\n",
      "       Dirty       0.94      0.97      0.95        30\n",
      "\n",
      "    accuracy                           0.95        60\n",
      "   macro avg       0.95      0.95      0.95        60\n",
      "weighted avg       0.95      0.95      0.95        60\n",
      "\n",
      "TabNet Confusion Matrix:\n",
      "[[28  2]\n",
      " [ 1 29]]\n",
      "\n",
      "=== Fold 4 (TabNet) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\kinzaenv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.23983 | train_accuracy: 0.46667 | val_accuracy: 0.4     |  0:00:00s\n",
      "epoch 1  | loss: 1.28311 | train_accuracy: 0.5     | val_accuracy: 0.48333 |  0:00:00s\n",
      "epoch 2  | loss: 1.15743 | train_accuracy: 0.52083 | val_accuracy: 0.51667 |  0:00:00s\n",
      "epoch 3  | loss: 1.02248 | train_accuracy: 0.5375  | val_accuracy: 0.55    |  0:00:00s\n",
      "epoch 4  | loss: 0.9849  | train_accuracy: 0.52917 | val_accuracy: 0.53333 |  0:00:01s\n",
      "epoch 5  | loss: 1.04068 | train_accuracy: 0.5625  | val_accuracy: 0.5     |  0:00:01s\n",
      "epoch 6  | loss: 0.78155 | train_accuracy: 0.58333 | val_accuracy: 0.53333 |  0:00:01s\n",
      "epoch 7  | loss: 0.82032 | train_accuracy: 0.59167 | val_accuracy: 0.48333 |  0:00:01s\n",
      "epoch 8  | loss: 0.64754 | train_accuracy: 0.65    | val_accuracy: 0.46667 |  0:00:01s\n",
      "epoch 9  | loss: 0.62641 | train_accuracy: 0.65    | val_accuracy: 0.61667 |  0:00:02s\n",
      "epoch 10 | loss: 0.63878 | train_accuracy: 0.7     | val_accuracy: 0.63333 |  0:00:02s\n",
      "epoch 11 | loss: 0.67168 | train_accuracy: 0.72917 | val_accuracy: 0.63333 |  0:00:02s\n",
      "epoch 12 | loss: 0.62364 | train_accuracy: 0.75417 | val_accuracy: 0.7     |  0:00:02s\n",
      "epoch 13 | loss: 0.5302  | train_accuracy: 0.76667 | val_accuracy: 0.76667 |  0:00:02s\n",
      "epoch 14 | loss: 0.5194  | train_accuracy: 0.77083 | val_accuracy: 0.75    |  0:00:03s\n",
      "epoch 15 | loss: 0.5533  | train_accuracy: 0.7625  | val_accuracy: 0.71667 |  0:00:03s\n",
      "epoch 16 | loss: 0.54971 | train_accuracy: 0.77917 | val_accuracy: 0.73333 |  0:00:03s\n",
      "epoch 17 | loss: 0.41307 | train_accuracy: 0.77083 | val_accuracy: 0.7     |  0:00:03s\n",
      "epoch 18 | loss: 0.42032 | train_accuracy: 0.7875  | val_accuracy: 0.73333 |  0:00:03s\n",
      "epoch 19 | loss: 0.40914 | train_accuracy: 0.8125  | val_accuracy: 0.71667 |  0:00:04s\n",
      "epoch 20 | loss: 0.37224 | train_accuracy: 0.80417 | val_accuracy: 0.7     |  0:00:04s\n",
      "epoch 21 | loss: 0.38331 | train_accuracy: 0.80833 | val_accuracy: 0.68333 |  0:00:04s\n",
      "epoch 22 | loss: 0.3701  | train_accuracy: 0.80417 | val_accuracy: 0.7     |  0:00:04s\n",
      "epoch 23 | loss: 0.32876 | train_accuracy: 0.82083 | val_accuracy: 0.71667 |  0:00:04s\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_accuracy = 0.76667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\kinzaenv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Best Validation Accuracy: 0.7667\n",
      "TabNet Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.74      0.83      0.78        30\n",
      "       Dirty       0.81      0.70      0.75        30\n",
      "\n",
      "    accuracy                           0.77        60\n",
      "   macro avg       0.77      0.77      0.77        60\n",
      "weighted avg       0.77      0.77      0.77        60\n",
      "\n",
      "TabNet Confusion Matrix:\n",
      "[[25  5]\n",
      " [ 9 21]]\n",
      "\n",
      "=== Fold 5 (TabNet) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\kinzaenv\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.32091 | train_accuracy: 0.475   | val_accuracy: 0.41667 |  0:00:00s\n",
      "epoch 1  | loss: 1.41698 | train_accuracy: 0.49583 | val_accuracy: 0.56667 |  0:00:00s\n",
      "epoch 2  | loss: 1.15508 | train_accuracy: 0.55417 | val_accuracy: 0.53333 |  0:00:00s\n",
      "epoch 3  | loss: 1.19257 | train_accuracy: 0.60833 | val_accuracy: 0.6     |  0:00:00s\n",
      "epoch 4  | loss: 0.86906 | train_accuracy: 0.6125  | val_accuracy: 0.65    |  0:00:01s\n",
      "epoch 5  | loss: 0.90439 | train_accuracy: 0.6375  | val_accuracy: 0.63333 |  0:00:01s\n",
      "epoch 6  | loss: 0.72653 | train_accuracy: 0.64583 | val_accuracy: 0.61667 |  0:00:01s\n",
      "epoch 7  | loss: 0.75137 | train_accuracy: 0.675   | val_accuracy: 0.66667 |  0:00:01s\n",
      "epoch 8  | loss: 0.67979 | train_accuracy: 0.72083 | val_accuracy: 0.68333 |  0:00:01s\n",
      "epoch 9  | loss: 0.577   | train_accuracy: 0.69167 | val_accuracy: 0.68333 |  0:00:02s\n",
      "epoch 10 | loss: 0.60418 | train_accuracy: 0.71667 | val_accuracy: 0.68333 |  0:00:02s\n",
      "epoch 11 | loss: 0.56015 | train_accuracy: 0.775   | val_accuracy: 0.71667 |  0:00:02s\n",
      "epoch 12 | loss: 0.4775  | train_accuracy: 0.74583 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 13 | loss: 0.50905 | train_accuracy: 0.75417 | val_accuracy: 0.75    |  0:00:02s\n",
      "epoch 14 | loss: 0.36648 | train_accuracy: 0.75417 | val_accuracy: 0.73333 |  0:00:03s\n",
      "epoch 15 | loss: 0.37155 | train_accuracy: 0.75833 | val_accuracy: 0.73333 |  0:00:03s\n",
      "epoch 16 | loss: 0.39612 | train_accuracy: 0.78333 | val_accuracy: 0.71667 |  0:00:03s\n",
      "epoch 17 | loss: 0.39137 | train_accuracy: 0.81667 | val_accuracy: 0.76667 |  0:00:03s\n",
      "epoch 18 | loss: 0.29278 | train_accuracy: 0.85833 | val_accuracy: 0.76667 |  0:00:03s\n",
      "epoch 19 | loss: 0.26253 | train_accuracy: 0.8375  | val_accuracy: 0.78333 |  0:00:03s\n",
      "epoch 20 | loss: 0.27642 | train_accuracy: 0.83333 | val_accuracy: 0.76667 |  0:00:04s\n",
      "epoch 21 | loss: 0.25523 | train_accuracy: 0.85417 | val_accuracy: 0.78333 |  0:00:04s\n",
      "epoch 22 | loss: 0.28004 | train_accuracy: 0.90417 | val_accuracy: 0.86667 |  0:00:04s\n",
      "epoch 23 | loss: 0.18694 | train_accuracy: 0.92917 | val_accuracy: 0.86667 |  0:00:04s\n",
      "epoch 24 | loss: 0.23214 | train_accuracy: 0.9375  | val_accuracy: 0.86667 |  0:00:04s\n",
      "epoch 25 | loss: 0.22474 | train_accuracy: 0.93333 | val_accuracy: 0.85    |  0:00:05s\n",
      "epoch 26 | loss: 0.1656  | train_accuracy: 0.94583 | val_accuracy: 0.9     |  0:00:05s\n",
      "epoch 27 | loss: 0.09538 | train_accuracy: 0.9625  | val_accuracy: 0.9     |  0:00:05s\n",
      "epoch 28 | loss: 0.09069 | train_accuracy: 0.95833 | val_accuracy: 0.9     |  0:00:05s\n",
      "epoch 29 | loss: 0.10551 | train_accuracy: 0.975   | val_accuracy: 0.93333 |  0:00:05s\n",
      "epoch 30 | loss: 0.06119 | train_accuracy: 0.975   | val_accuracy: 0.91667 |  0:00:06s\n",
      "epoch 31 | loss: 0.08932 | train_accuracy: 0.9875  | val_accuracy: 0.9     |  0:00:06s\n",
      "epoch 32 | loss: 0.05743 | train_accuracy: 0.9875  | val_accuracy: 0.93333 |  0:00:06s\n",
      "epoch 33 | loss: 0.05179 | train_accuracy: 0.9875  | val_accuracy: 0.95    |  0:00:06s\n",
      "epoch 34 | loss: 0.07581 | train_accuracy: 0.99583 | val_accuracy: 0.98333 |  0:00:06s\n",
      "epoch 35 | loss: 0.08617 | train_accuracy: 0.99167 | val_accuracy: 0.98333 |  0:00:07s\n",
      "epoch 36 | loss: 0.07876 | train_accuracy: 0.99583 | val_accuracy: 0.95    |  0:00:07s\n",
      "epoch 37 | loss: 0.01847 | train_accuracy: 0.9875  | val_accuracy: 0.98333 |  0:00:07s\n",
      "epoch 38 | loss: 0.03595 | train_accuracy: 0.9875  | val_accuracy: 1.0     |  0:00:07s\n",
      "epoch 39 | loss: 0.03589 | train_accuracy: 0.99167 | val_accuracy: 1.0     |  0:00:07s\n",
      "epoch 40 | loss: 0.02434 | train_accuracy: 0.99167 | val_accuracy: 1.0     |  0:00:08s\n",
      "epoch 41 | loss: 0.03011 | train_accuracy: 0.99583 | val_accuracy: 1.0     |  0:00:08s\n",
      "epoch 42 | loss: 0.14017 | train_accuracy: 1.0     | val_accuracy: 1.0     |  0:00:08s\n",
      "epoch 43 | loss: 0.05891 | train_accuracy: 1.0     | val_accuracy: 0.98333 |  0:00:08s\n",
      "epoch 44 | loss: 0.02794 | train_accuracy: 0.99583 | val_accuracy: 0.98333 |  0:00:08s\n",
      "epoch 45 | loss: 0.02081 | train_accuracy: 0.99583 | val_accuracy: 0.98333 |  0:00:08s\n",
      "epoch 46 | loss: 0.01602 | train_accuracy: 1.0     | val_accuracy: 0.98333 |  0:00:09s\n",
      "epoch 47 | loss: 0.01126 | train_accuracy: 1.0     | val_accuracy: 1.0     |  0:00:09s\n",
      "epoch 48 | loss: 0.00524 | train_accuracy: 1.0     | val_accuracy: 0.98333 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 48 with best_epoch = 38 and best_val_accuracy = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\kinzaenv\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Best Validation Accuracy: 1.0000\n",
      "TabNet Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       1.00      1.00      1.00        30\n",
      "       Dirty       1.00      1.00      1.00        30\n",
      "\n",
      "    accuracy                           1.00        60\n",
      "   macro avg       1.00      1.00      1.00        60\n",
      "weighted avg       1.00      1.00      1.00        60\n",
      "\n",
      "TabNet Confusion Matrix:\n",
      "[[30  0]\n",
      " [ 0 30]]\n",
      "✅ All plots saved in 'plots' directory\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# Load features and labels\n",
    "X_resampled = np.load('X_resampled.npy')\n",
    "y_resampled = np.load('y_resampled.npy')\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_resampled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "# Create output directory\n",
    "#os.makedirs('saved_models', exist_ok=True)\n",
    "os.makedirs('PLOTS_TabNet', exist_ok=True)  # Directory for saving plots\n",
    "\n",
    "# Prepare 5-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Running 5-fold cross-validation with TabNet...\")\n",
    "\n",
    "# Lists to store validation accuracy curves for all folds\n",
    "all_val_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_resampled, y_resampled)):\n",
    "    X_train, X_val = X_resampled[train_idx], X_resampled[val_idx]\n",
    "    y_train, y_val = y_resampled[train_idx], y_resampled[val_idx]\n",
    "\n",
    "    print(f\"\\n=== Fold {fold + 1} (TabNet) ===\")\n",
    "\n",
    "    # Initialize TabNet\n",
    "    tabnet_model = TabNetClassifier(\n",
    "        n_d=64,  # Width of decision prediction layer\n",
    "        n_a=64,  # Width of attention embedding\n",
    "        n_steps=5,  # Number of attention steps\n",
    "        gamma=1.3,  # Scaling factor for attention updates\n",
    "        lambda_sparse=1e-3,  # Sparsity regularization\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=2e-2),\n",
    "        scheduler_params={\"step_size\": 10, \"gamma\": 0.9},\n",
    "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "        mask_type='entmax',  # Sparsemax for attention\n",
    "        device_name=str(device)\n",
    "    )\n",
    "\n",
    "    # Train TabNet\n",
    "    tabnet_model.fit(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "        eval_name=['train', 'val'],\n",
    "        eval_metric=['accuracy'],\n",
    "        max_epochs=100,\n",
    "        patience=10,  # Early stopping\n",
    "        batch_size=256,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    # Extract training history\n",
    "    history = tabnet_model.history\n",
    "    train_acc = history['train_accuracy']\n",
    "    val_acc = history['val_accuracy']\n",
    "    epochs = range(1, len(train_acc) + 1)\n",
    "\n",
    "    # Store validation accuracy for combined plot\n",
    "    all_val_accuracies.append(val_acc)\n",
    "\n",
    "    # Plot accuracy per epoch for this fold\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_acc, label='Train Accuracy', color='blue')\n",
    "    plt.plot(epochs, val_acc, label='Validation Accuracy', color='orange')\n",
    "    plt.title(f'Fold {fold + 1} - Accuracy per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'PLOTS_TabNet/accuracy_fold_{fold + 1}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Evaluate on validation data\n",
    "    val_pred = tabnet_model.predict(X_val)\n",
    "    val_acc_final = accuracy_score(y_val, val_pred)\n",
    "\n",
    "    print(f\"Fold {fold + 1} Best Validation Accuracy: {val_acc_final:.4f}\")\n",
    "    print(\"TabNet Classification Report:\")\n",
    "    print(classification_report(y_val, val_pred, target_names=['Clean', 'Dirty']))\n",
    "    print(\"TabNet Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, val_pred))\n",
    "\n",
    "    # Plot feature importance\n",
    "    feature_importances = tabnet_model.feature_importances_\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(feature_importances)), feature_importances)\n",
    "    plt.title(f'Fold {fold + 1} - Feature Importance')\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'PLOTS_TabNet/feature_importance_fold_{fold + 1}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Save the model\n",
    "    # model_path = f\"saved_models/best_tabnet_fold_{fold + 1}.joblib\"\n",
    "    # joblib.dump(tabnet_model, model_path)\n",
    "    # print(f\"✅ Saved best TabNet model for Fold {fold + 1} to: {model_path}\")\n",
    "\n",
    "# Plot combined validation accuracy for all folds\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['blue', 'orange', 'green', 'red', 'purple']\n",
    "for fold, val_acc in enumerate(all_val_accuracies):\n",
    "    plt.plot(range(1, len(val_acc) + 1), val_acc, label=f'Fold {fold + 1}', color=colors[fold])\n",
    "plt.title('Validation Accuracy Across All Folds')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('PLOTS_TabNet/combined_validation_accuracy.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"✅ All plots saved in 'plots' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6e4c12",
   "metadata": {},
   "source": [
    "### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe44fae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n",
      "\n",
      "=== Fold 1 ===\n",
      "Training with lr=0.001, batch_size=32, dropout=0.1, class_weight=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 lr=0.001 bs=32 drop=0.1 weight=1.0:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 lr=0.001 bs=32 drop=0.1 weight=1.0:   0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x1025 and 1024x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 114\u001b[0m\n\u001b[0;32m    112\u001b[0m X_batch, y_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    113\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 114\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[0;32m    116\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\kinzaenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\kinzaenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[27], line 36\u001b[0m, in \u001b[0;36mResNetClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\kinzaenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\kinzaenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\kinzaenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\kinzaenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\kinzaenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\kinzaenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x1025 and 1024x256)"
     ]
    }
   ],
   "source": [
    "# Install dependencies: pip install numpy pandas scikit-learn matplotlib joblib tqdm torch torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "# Set Matplotlib backend to 'Agg' for non-interactive environments\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# Define custom ResNet-50 classifier for pre-extracted features\n",
    "class ResNetClassifier(nn.Module):\n",
    "    def __init__(self, hidden_dim=1024, num_classes=2, hidden_size=256, dropout=0.1):\n",
    "        super(ResNetClassifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Load features and labels\n",
    "X_resampled = np.load('X_resampled.npy')  # Shape: (3000, 1024) for features\n",
    "y_resampled = np.load('y_resampled.npy')  # Shape: (3000,) with 0=Clean, 1=Dirty\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_resampled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "# Create output directories\n",
    "#os.makedirs('saved_models_ResNet', exist_ok=True)\n",
    "os.makedirs('PLOTS_ResNet', exist_ok=True)\n",
    "\n",
    "# 5-Fold Stratified Cross-Validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "all_train_acc = []\n",
    "all_val_acc = []\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-3, 5e-4, 1e-4],\n",
    "    'batch_size': [32, 64],\n",
    "    'dropout': [0.1, 0.2],\n",
    "    'class_weight': [1.0, 1.5]  # Weight for Dirty class\n",
    "}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_resampled, y_resampled)):\n",
    "    print(f\"\\n=== Fold {fold + 1} ===\")\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    best_model_state = None\n",
    "    best_val_preds = None\n",
    "    best_train_acc_per_epoch = []\n",
    "    best_val_acc_per_epoch = []\n",
    "    best_params = None\n",
    "    \n",
    "    # Hyperparameter tuning\n",
    "    for lr, bs, dropout, weight in product(param_grid['learning_rate'], param_grid['batch_size'], param_grid['dropout'], param_grid['class_weight']):\n",
    "        print(f\"Training with lr={lr}, batch_size={bs}, dropout={dropout}, class_weight={weight}\")\n",
    "        \n",
    "        # Prepare data\n",
    "        X_train, X_val = X_resampled[train_idx], X_resampled[val_idx]\n",
    "        y_train, y_val = y_resampled[train_idx], y_resampled[val_idx]\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "        \n",
    "        # Create DataLoaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=bs)\n",
    "        \n",
    "        # Initialize model, loss, optimizer, and scheduler\n",
    "        model = ResNetClassifier(hidden_dim=1024, num_classes=2, hidden_size=256, dropout=dropout).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, weight]).to(device))\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "        \n",
    "        # Training loop with early stopping\n",
    "        no_improve_count = 0\n",
    "        train_acc_per_epoch = []\n",
    "        val_acc_per_epoch = []\n",
    "        patience = 20\n",
    "        \n",
    "        for epoch in tqdm(range(100), desc=f'Fold {fold + 1} lr={lr} bs={bs} drop={dropout} weight={weight}'):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += y_batch.size(0)\n",
    "                train_correct += (predicted == y_batch).sum().item()\n",
    "            train_acc = train_correct / train_total\n",
    "            train_acc_per_epoch.append(train_acc)\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            val_preds = []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    outputs = model(X_batch)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += y_batch.size(0)\n",
    "                    val_correct += (predicted == y_batch).sum().item()\n",
    "                    val_preds.extend(predicted.cpu().numpy())\n",
    "            val_acc = val_correct / val_total\n",
    "            val_acc_per_epoch.append(val_acc)\n",
    "            \n",
    "            # Scheduler step\n",
    "            scheduler.step(val_acc)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model_state = model.state_dict()\n",
    "                best_val_preds = val_preds\n",
    "                best_train_acc_per_epoch = train_acc_per_epoch.copy()\n",
    "                best_val_acc_per_epoch = val_acc_per_epoch.copy()\n",
    "                best_params = {'lr': lr, 'batch_size': bs, 'dropout': dropout, 'class_weight': weight}\n",
    "                no_improve_count = 0\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "            \n",
    "            if no_improve_count >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} with params {best_params}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"Params lr={lr}, bs={bs}, drop={dropout}, weight={weight}: Best Val Acc = {best_val_acc:.4f}\")\n",
    "    \n",
    "    # Store fold accuracy\n",
    "    fold_accuracies.append(best_val_acc)\n",
    "    all_train_acc.append(best_train_acc_per_epoch)\n",
    "    all_val_acc.append(best_val_acc_per_epoch)\n",
    "    \n",
    "    # Plot training and validation accuracy for this fold\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(best_train_acc_per_epoch) + 1), best_train_acc_per_epoch, label='Train Accuracy', color='blue')\n",
    "    plt.plot(range(1, len(best_val_acc_per_epoch) + 1), best_val_acc_per_epoch, label='Validation Accuracy', color='orange')\n",
    "    plt.title(f'Fold {fold + 1} - ResNet Accuracy (Params: {best_params})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0.8, 1.0)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'PLOTS_ResNet/resnet_accuracy_fold_{fold + 1}.png')\n",
    "    plt.close()\n",
    "    print(f\"Fold {fold + 1}: Saved accuracy plot to plots_ResNet/resnet_accuracy_fold_{fold + 1}.png\")\n",
    "    \n",
    "    # Classification report and confusion matrix\n",
    "    print(f\"\\nFold {fold + 1} Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_val, best_val_preds, target_names=['Clean', 'Dirty']))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, best_val_preds))\n",
    "    \n",
    "    # Save the best model\n",
    "    #model_path = f\"saved_models_ResNet/resnet_fold_{fold + 1}.pt\"\n",
    "    #torch.save(best_model_state, model_path)\n",
    "    #print(f\"Fold {fold + 1}: Saved ResNet model to {model_path}\")\n",
    "\n",
    "# Combined validation accuracy plot for all folds (only validation accuracy)\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "for fold in range(5):\n",
    "    plt.plot(range(1, len(all_val_acc[fold]) + 1), all_val_acc[fold], \n",
    "             label=f'Fold {fold + 1} Val', color=colors[fold], linestyle='-')\n",
    "plt.title('ResNet Validation Accuracy Across All Folds')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.8, 1.0)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('PLOTS_ResNet/resnet_combined_accuracy.png')\n",
    "plt.close()\n",
    "print(\"Saved combined validation accuracy plot to plots_ResNet/resnet_combined_accuracy.png\")\n",
    "\n",
    "# Final summary\n",
    "final_best_accuracy = max(fold_accuracies)\n",
    "print(f\"\\nFinal Best Accuracy (across all folds): {final_best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8ea82b",
   "metadata": {},
   "source": [
    "### ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d381d2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n",
      "\n",
      "=== Fold 1 ===\n",
      "Training with lr=0.001, batch_size=32, dropout=0.1, class_weight=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 lr=0.001 bs=32 drop=0.1 weight=1.0:  45%|████▌     | 45/100 [00:09<00:12,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 46 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1, 'class_weight': 1.0}\n",
      "Params lr=0.001, bs=32, drop=0.1, weight=1.0: Best Val Acc = 0.9281\n",
      "Training with lr=0.001, batch_size=32, dropout=0.1, class_weight=1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 lr=0.001 bs=32 drop=0.1 weight=1.5:  19%|█▉        | 19/100 [00:04<00:18,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 20 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1, 'class_weight': 1.0}\n",
      "Params lr=0.001, bs=32, drop=0.1, weight=1.5: Best Val Acc = 0.9281\n",
      "Training with lr=0.001, batch_size=32, dropout=0.2, class_weight=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 lr=0.001 bs=32 drop=0.2 weight=1.0:  19%|█▉        | 19/100 [00:03<00:16,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 20 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1, 'class_weight': 1.0}\n",
      "Params lr=0.001, bs=32, drop=0.2, weight=1.0: Best Val Acc = 0.9281\n",
      "Training with lr=0.001, batch_size=32, dropout=0.2, class_weight=1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 lr=0.001 bs=32 drop=0.2 weight=1.5:  19%|█▉        | 19/100 [00:04<00:17,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 20 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1, 'class_weight': 1.0}\n",
      "Params lr=0.001, bs=32, drop=0.2, weight=1.5: Best Val Acc = 0.9281\n",
      "Training with lr=0.001, batch_size=64, dropout=0.1, class_weight=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 lr=0.001 bs=64 drop=0.1 weight=1.0:  11%|█         | 11/100 [00:01<00:10,  8.52it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 111\u001b[0m\n\u001b[0;32m    109\u001b[0m train_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    110\u001b[0m train_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m    112\u001b[0m     X_batch, y_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    113\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Install dependencies: pip install numpy pandas scikit-learn matplotlib joblib tqdm torch transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "# Set Matplotlib backend to 'Agg' for non-interactive environments\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# Define custom ViT classification head for pre-extracted features\n",
    "class ViTClassifier(nn.Module):\n",
    "    def __init__(self, hidden_dim=1024, num_classes=2, hidden_size=256, dropout=0.1):\n",
    "        super(ViTClassifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Load features and labels\n",
    "X_resampled = np.load('X_resampled.npy')  # Shape: (3000, 1024) for ViT features\n",
    "y_resampled = np.load('y_resampled.npy')  # Shape: (3000,) with 0=Clean, 1=Dirty\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_resampled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "# Create output directories\n",
    "#os.makedirs('saved_models_ViT', exist_ok=True)\n",
    "os.makedirs('PLOTS_ViT', exist_ok=True)\n",
    "\n",
    "# 5-Fold Stratified Cross-Validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "all_train_acc = []\n",
    "all_val_acc = []\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-3, 5e-4, 1e-4],\n",
    "    'batch_size': [32, 64],\n",
    "    'dropout': [0.1, 0.2],\n",
    "    'class_weight': [1.0, 1.5]  # Weight for Dirty class\n",
    "}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_resampled, y_resampled)):\n",
    "    print(f\"\\n=== Fold {fold + 1} ===\")\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    best_model_state = None\n",
    "    best_val_preds = None\n",
    "    best_train_acc_per_epoch = []\n",
    "    best_val_acc_per_epoch = []\n",
    "    best_params = None\n",
    "    \n",
    "    # Hyperparameter tuning\n",
    "    for lr, bs, dropout, weight in product(param_grid['learning_rate'], param_grid['batch_size'], param_grid['dropout'], param_grid['class_weight']):\n",
    "        print(f\"Training with lr={lr}, batch_size={bs}, dropout={dropout}, class_weight={weight}\")\n",
    "        \n",
    "        # Prepare data\n",
    "        X_train, X_val = X_resampled[train_idx], X_resampled[val_idx]\n",
    "        y_train, y_val = y_resampled[train_idx], y_resampled[val_idx]\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "        \n",
    "        # Create DataLoaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=bs)\n",
    "        \n",
    "        # Initialize model, loss, optimizer, and scheduler\n",
    "        model = ViTClassifier(hidden_dim=1024, num_classes=2, hidden_size=256, dropout=dropout).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, weight]).to(device))\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "        \n",
    "        # Training loop with early stopping\n",
    "        no_improve_count = 0\n",
    "        train_acc_per_epoch = []\n",
    "        val_acc_per_epoch = []\n",
    "        patience = 20\n",
    "        \n",
    "        for epoch in tqdm(range(100), desc=f'Fold {fold + 1} lr={lr} bs={bs} drop={dropout} weight={weight}'):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += y_batch.size(0)\n",
    "                train_correct += (predicted == y_batch).sum().item()\n",
    "            train_acc = train_correct / train_total\n",
    "            train_acc_per_epoch.append(train_acc)\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            val_preds = []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    outputs = model(X_batch)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += y_batch.size(0)\n",
    "                    val_correct += (predicted == y_batch).sum().item()\n",
    "                    val_preds.extend(predicted.cpu().numpy())\n",
    "            val_acc = val_correct / val_total\n",
    "            val_acc_per_epoch.append(val_acc)\n",
    "            \n",
    "            # Scheduler step\n",
    "            scheduler.step(val_acc)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model_state = model.state_dict()\n",
    "                best_val_preds = val_preds\n",
    "                best_train_acc_per_epoch = train_acc_per_epoch.copy()\n",
    "                best_val_acc_per_epoch = val_acc_per_epoch.copy()\n",
    "                best_params = {'lr': lr, 'batch_size': bs, 'dropout': dropout, 'class_weight': weight}\n",
    "                no_improve_count = 0\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "            \n",
    "            if no_improve_count >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} with params {best_params}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"Params lr={lr}, bs={bs}, drop={dropout}, weight={weight}: Best Val Acc = {best_val_acc:.4f}\")\n",
    "    \n",
    "    # Store fold accuracy\n",
    "    fold_accuracies.append(best_val_acc)\n",
    "    all_train_acc.append(best_train_acc_per_epoch)\n",
    "    all_val_acc.append(best_val_acc_per_epoch)\n",
    "    \n",
    "    # Plot training and validation accuracy for this fold\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(best_train_acc_per_epoch) + 1), best_train_acc_per_epoch, label='Train Accuracy', color='blue')\n",
    "    plt.plot(range(1, len(best_val_acc_per_epoch) + 1), best_val_acc_per_epoch, label='Validation Accuracy', color='orange')\n",
    "    plt.title(f'Fold {fold + 1} - ViT Accuracy over Epochs (Params: {best_params})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0.8, 1.0)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'PLOTS_ViT/vit_accuracy_fold_{fold + 1}.png')\n",
    "    plt.close()\n",
    "    print(f\"Fold {fold + 1}: Saved accuracy plot to plots_ViT/vit_accuracy_fold_{fold + 1}.png\")\n",
    "    \n",
    "    # Classification report and confusion matrix\n",
    "    print(f\"\\nFold {fold + 1} Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_val, best_val_preds, target_names=['Clean', 'Dirty']))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, best_val_preds))\n",
    "    \n",
    "    # Save the best model\n",
    "    #model_path = f\"saved_models_ViT/vit_fold_{fold + 1}.pt\"\n",
    "    #torch.save(best_model_state, model_path)\n",
    "    #print(f\"Fold {fold + 1}: Saved ViT model to {model_path}\")\n",
    "\n",
    "# Combined validation accuracy plot for all folds (only validation accuracy)\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "for fold in range(5):\n",
    "    plt.plot(range(1, len(all_val_acc[fold]) + 1), all_val_acc[fold], \n",
    "             label=f'Fold {fold + 1} Val', color=colors[fold], linestyle='-')\n",
    "plt.title('ViT Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.8, 1.0)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('PLOTS_ViT/vit_combined_accuracy.png')\n",
    "plt.close()\n",
    "print(\"Saved combined validation accuracy plot to plots_ViT/vit_combined_accuracy.png\")\n",
    "\n",
    "# Final summary\n",
    "final_best_accuracy = max(fold_accuracies)\n",
    "print(f\"\\nFinal Best Accuracy (across all folds): {final_best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294e5d2e",
   "metadata": {},
   "source": [
    "### ViT - Head (ViT-Head 1) - Best Accuracy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22236169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n",
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1:  27%|██▋       | 27/100 [00:01<00:04, 16.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 28\n",
      "Fold 1: Saved accuracy plot to plots_ViTHead_1/vithead_accuracy_fold_1.png\n",
      "\n",
      "Fold 1 Validation Accuracy: 0.8997\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.89      0.91      0.90       299\n",
      "       Dirty       0.91      0.89      0.90       299\n",
      "\n",
      "    accuracy                           0.90       598\n",
      "   macro avg       0.90      0.90      0.90       598\n",
      "weighted avg       0.90      0.90      0.90       598\n",
      "\n",
      "Confusion Matrix:\n",
      "[[271  28]\n",
      " [ 32 267]]\n",
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2:  14%|█▍        | 14/100 [00:00<00:05, 14.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15\n",
      "Fold 2: Saved accuracy plot to plots_ViTHead_1/vithead_accuracy_fold_2.png\n",
      "\n",
      "Fold 2 Validation Accuracy: 0.8727\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.87      0.87      0.87       299\n",
      "       Dirty       0.87      0.87      0.87       298\n",
      "\n",
      "    accuracy                           0.87       597\n",
      "   macro avg       0.87      0.87      0.87       597\n",
      "weighted avg       0.87      0.87      0.87       597\n",
      "\n",
      "Confusion Matrix:\n",
      "[[261  38]\n",
      " [ 38 260]]\n",
      "\n",
      "=== Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3:  27%|██▋       | 27/100 [00:01<00:04, 17.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 28\n",
      "Fold 3: Saved accuracy plot to plots_ViTHead_1/vithead_accuracy_fold_3.png\n",
      "\n",
      "Fold 3 Validation Accuracy: 0.8894\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.91      0.86      0.89       299\n",
      "       Dirty       0.87      0.92      0.89       298\n",
      "\n",
      "    accuracy                           0.89       597\n",
      "   macro avg       0.89      0.89      0.89       597\n",
      "weighted avg       0.89      0.89      0.89       597\n",
      "\n",
      "Confusion Matrix:\n",
      "[[258  41]\n",
      " [ 25 273]]\n",
      "\n",
      "=== Fold 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4:  30%|███       | 30/100 [00:01<00:03, 19.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 31\n",
      "Fold 4: Saved accuracy plot to plots_ViTHead_1/vithead_accuracy_fold_4.png\n",
      "\n",
      "Fold 4 Validation Accuracy: 0.8894\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.89      0.88      0.89       298\n",
      "       Dirty       0.88      0.90      0.89       299\n",
      "\n",
      "    accuracy                           0.89       597\n",
      "   macro avg       0.89      0.89      0.89       597\n",
      "weighted avg       0.89      0.89      0.89       597\n",
      "\n",
      "Confusion Matrix:\n",
      "[[263  35]\n",
      " [ 31 268]]\n",
      "\n",
      "=== Fold 5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5:  17%|█▋        | 17/100 [00:00<00:04, 17.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 18\n",
      "Fold 5: Saved accuracy plot to plots_ViTHead_1/vithead_accuracy_fold_5.png\n",
      "\n",
      "Fold 5 Validation Accuracy: 0.8878\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.91      0.86      0.88       298\n",
      "       Dirty       0.87      0.91      0.89       299\n",
      "\n",
      "    accuracy                           0.89       597\n",
      "   macro avg       0.89      0.89      0.89       597\n",
      "weighted avg       0.89      0.89      0.89       597\n",
      "\n",
      "Confusion Matrix:\n",
      "[[257  41]\n",
      " [ 26 273]]\n",
      "Saved combined accuracy plot to plots_ViTHead_1/vithead_combined_accuracy.png\n",
      "\n",
      "Final Best Accuracy (across all folds): 0.8997\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies: pip install numpy pandas scikit-learn matplotlib joblib tqdm torch transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# Set Matplotlib backend to 'Agg' for non-interactive environments\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Define ViT classification head\n",
    "class ViTHead(nn.Module):\n",
    "    def __init__(self, hidden_dim=1024, num_classes=2):\n",
    "        super(ViTHead, self).__init__()\n",
    "        self.head = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.head(x)\n",
    "\n",
    "# Load features and labels\n",
    "X_resampled = np.load('X_resampled.npy')  # Shape: (3000, 1024) for ViT features\n",
    "y_resampled = np.load('y_resampled.npy')  # Shape: (3000,) with 0=Clean, 1=Dirty\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_resampled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "# Create output directories\n",
    "#os.makedirs('saved_models_ViTHead_1', exist_ok=True)\n",
    "os.makedirs('PLOTS_ViTHead_1', exist_ok=True)\n",
    "\n",
    "# 5-Fold Stratified Cross-Validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "all_train_acc = []\n",
    "all_val_acc = []\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "max_epochs = 100\n",
    "patience = 20\n",
    "learning_rate = 1e-3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_resampled, y_resampled)):\n",
    "    print(f\"\\n=== Fold {fold + 1} ===\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train, X_val = X_resampled[train_idx], X_resampled[val_idx]\n",
    "    y_train, y_val = y_resampled[train_idx], y_resampled[val_idx]\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model, loss, and optimizer\n",
    "    model = ViTHead(hidden_dim=1024, num_classes=2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    best_val_acc = 0\n",
    "    no_improve_count = 0\n",
    "    train_acc_per_epoch = []\n",
    "    val_acc_per_epoch = []\n",
    "    \n",
    "    for epoch in tqdm(range(max_epochs), desc=f'Fold {fold + 1}'):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += y_batch.size(0)\n",
    "            train_correct += (predicted == y_batch).sum().item()\n",
    "        train_acc = train_correct / train_total\n",
    "        train_acc_per_epoch.append(train_acc)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_preds = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += y_batch.size(0)\n",
    "                val_correct += (predicted == y_batch).sum().item()\n",
    "                val_preds.extend(predicted.cpu().numpy())\n",
    "        val_acc = val_correct / val_total\n",
    "        val_acc_per_epoch.append(val_acc)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict()\n",
    "            best_val_preds = val_preds\n",
    "            no_improve_count = 0\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "        \n",
    "        if no_improve_count >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "    # Store fold accuracy\n",
    "    fold_accuracies.append(best_val_acc)\n",
    "    all_train_acc.append(train_acc_per_epoch)\n",
    "    all_val_acc.append(val_acc_per_epoch)\n",
    "    \n",
    "    # Plot training and validation accuracy for this fold\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_acc_per_epoch) + 1), train_acc_per_epoch, label='Train Accuracy', color='blue')\n",
    "    plt.plot(range(1, len(val_acc_per_epoch) + 1), val_acc_per_epoch, label='Validation Accuracy', color='orange')\n",
    "    plt.title(f'Fold {fold + 1} - ViTHead Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0.8, 1.0)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'PLOTS_ViTHead_1/vithead_accuracy_fold_{fold + 1}.png')\n",
    "    plt.close()\n",
    "    print(f\"Fold {fold + 1}: Saved accuracy plot to plots_ViTHead_1/vithead_accuracy_fold_{fold + 1}.png\")\n",
    "    \n",
    "    # Classification report and confusion matrix\n",
    "    print(f\"\\nFold {fold + 1} Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_val, best_val_preds, target_names=['Clean', 'Dirty']))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, best_val_preds))\n",
    "    \n",
    "    # Save the best model\n",
    "    #model_path = f\"saved_models_ViTHead_1/vithead_fold_{fold + 1}.pt\"\n",
    "    #torch.save(best_model_state, model_path)\n",
    "    #print(f\"Fold {fold + 1}: Saved ViTHead model to {model_path}\")\n",
    "\n",
    "# Combined accuracy plot for all folds\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "for fold in range(5):\n",
    "    plt.plot(range(1, len(all_train_acc[fold]) + 1), all_train_acc[fold], \n",
    "             label=f'Fold {fold + 1} Train', color=colors[fold], linestyle='--')\n",
    "    plt.plot(range(1, len(all_val_acc[fold]) + 1), all_val_acc[fold], \n",
    "             label=f'Fold {fold + 1} Val', color=colors[fold], linestyle='-')\n",
    "plt.title('ViTHead Accuracy Across All Folds')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.8, 1.0)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('PLOTS_ViTHead_1/vithead_combined_accuracy.png')\n",
    "plt.close()\n",
    "print(\"Saved combined accuracy plot to plots_ViTHead_1/vithead_combined_accuracy.png\")\n",
    "\n",
    "# Final summary\n",
    "final_best_accuracy = max(fold_accuracies)\n",
    "print(f\"\\nFinal Best Accuracy (across all folds): {final_best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b11148d",
   "metadata": {},
   "source": [
    "### Combine ViT head with MLP/SVM for a ~1–2% boost (ViT-Head 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f320fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n",
      "Training with lr=0.001, batch_size=32, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 lr=0.001 bs=32 drop=0.1:  31%|███       | 31/100 [00:03<00:08,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 32 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.001, bs=32, drop=0.1: Best Val Acc = 0.9247\n",
      "Training with lr=0.001, batch_size=32, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 lr=0.001 bs=32 drop=0.2:  14%|█▍        | 14/100 [00:01<00:09,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.001, bs=32, drop=0.2: Best Val Acc = 0.9247\n",
      "Training with lr=0.001, batch_size=64, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 lr=0.001 bs=64 drop=0.1:  14%|█▍        | 14/100 [00:01<00:07, 11.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.001, bs=64, drop=0.1: Best Val Acc = 0.9247\n",
      "Training with lr=0.001, batch_size=64, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 lr=0.001 bs=64 drop=0.2:  25%|██▌       | 25/100 [00:03<00:11,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 26 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.001, bs=64, drop=0.2: Best Val Acc = 0.9298\n",
      "Training with lr=0.0005, batch_size=32, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 lr=0.0005 bs=32 drop=0.1:  14%|█▍        | 14/100 [00:03<00:21,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.0005, bs=32, drop=0.1: Best Val Acc = 0.9298\n",
      "Training with lr=0.0005, batch_size=32, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 lr=0.0005 bs=32 drop=0.2:  14%|█▍        | 14/100 [00:03<00:21,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.0005, bs=32, drop=0.2: Best Val Acc = 0.9298\n",
      "Training with lr=0.0005, batch_size=64, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 lr=0.0005 bs=64 drop=0.1:  14%|█▍        | 14/100 [00:02<00:14,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.0005, bs=64, drop=0.1: Best Val Acc = 0.9298\n",
      "Training with lr=0.0005, batch_size=64, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 lr=0.0005 bs=64 drop=0.2:  14%|█▍        | 14/100 [00:01<00:12,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.0005, bs=64, drop=0.2: Best Val Acc = 0.9298\n",
      "Training with lr=0.0001, batch_size=32, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 lr=0.0001 bs=32 drop=0.1:  14%|█▍        | 14/100 [00:03<00:20,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.0001, bs=32, drop=0.1: Best Val Acc = 0.9298\n",
      "Training with lr=0.0001, batch_size=32, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 lr=0.0001 bs=32 drop=0.2:  14%|█▍        | 14/100 [00:03<00:21,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.0001, bs=32, drop=0.2: Best Val Acc = 0.9298\n",
      "Training with lr=0.0001, batch_size=64, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 lr=0.0001 bs=64 drop=0.1:  14%|█▍        | 14/100 [00:02<00:13,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.0001, bs=64, drop=0.1: Best Val Acc = 0.9298\n",
      "Training with lr=0.0001, batch_size=64, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 lr=0.0001 bs=64 drop=0.2:  14%|█▍        | 14/100 [00:02<00:13,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.0001, bs=64, drop=0.2: Best Val Acc = 0.9298\n",
      "Fold 1: Saved accuracy plot to plots_ViTHead/vithead_accuracy_fold_1.png\n",
      "\n",
      "Fold 1 Validation Accuracy: 0.9298\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.93      0.93      0.93       299\n",
      "       Dirty       0.93      0.93      0.93       299\n",
      "\n",
      "    accuracy                           0.93       598\n",
      "   macro avg       0.93      0.93      0.93       598\n",
      "weighted avg       0.93      0.93      0.93       598\n",
      "\n",
      "Confusion Matrix:\n",
      "[[278  21]\n",
      " [ 21 278]]\n",
      "\n",
      "=== Fold 2 ===\n",
      "Training with lr=0.001, batch_size=32, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 lr=0.001 bs=32 drop=0.1:  17%|█▋        | 17/100 [00:04<00:20,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 18 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.001, bs=32, drop=0.1: Best Val Acc = 0.8894\n",
      "Training with lr=0.001, batch_size=32, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 lr=0.001 bs=32 drop=0.2:  14%|█▍        | 14/100 [00:03<00:23,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.001, bs=32, drop=0.2: Best Val Acc = 0.8894\n",
      "Training with lr=0.001, batch_size=64, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 lr=0.001 bs=64 drop=0.1:  14%|█▍        | 14/100 [00:02<00:14,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.001, bs=64, drop=0.1: Best Val Acc = 0.8894\n",
      "Training with lr=0.001, batch_size=64, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 lr=0.001 bs=64 drop=0.2:  29%|██▉       | 29/100 [00:04<00:10,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 30 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.001, bs=64, drop=0.2: Best Val Acc = 0.8911\n",
      "Training with lr=0.0005, batch_size=32, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 lr=0.0005 bs=32 drop=0.1:  14%|█▍        | 14/100 [00:03<00:21,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.0005, bs=32, drop=0.1: Best Val Acc = 0.8911\n",
      "Training with lr=0.0005, batch_size=32, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 lr=0.0005 bs=32 drop=0.2:  14%|█▍        | 14/100 [00:03<00:22,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.0005, bs=32, drop=0.2: Best Val Acc = 0.8911\n",
      "Training with lr=0.0005, batch_size=64, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 lr=0.0005 bs=64 drop=0.1:  21%|██        | 21/100 [00:03<00:13,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 22 with params {'lr': 0.0005, 'batch_size': 64, 'dropout': 0.1}\n",
      "Params lr=0.0005, bs=64, drop=0.1: Best Val Acc = 0.8961\n",
      "Training with lr=0.0005, batch_size=64, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 lr=0.0005 bs=64 drop=0.2:  14%|█▍        | 14/100 [00:02<00:14,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.0005, 'batch_size': 64, 'dropout': 0.1}\n",
      "Params lr=0.0005, bs=64, drop=0.2: Best Val Acc = 0.8961\n",
      "Training with lr=0.0001, batch_size=32, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 lr=0.0001 bs=32 drop=0.1:  14%|█▍        | 14/100 [00:03<00:21,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.0005, 'batch_size': 64, 'dropout': 0.1}\n",
      "Params lr=0.0001, bs=32, drop=0.1: Best Val Acc = 0.8961\n",
      "Training with lr=0.0001, batch_size=32, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 lr=0.0001 bs=32 drop=0.2:  14%|█▍        | 14/100 [00:03<00:23,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.0005, 'batch_size': 64, 'dropout': 0.1}\n",
      "Params lr=0.0001, bs=32, drop=0.2: Best Val Acc = 0.8961\n",
      "Training with lr=0.0001, batch_size=64, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 lr=0.0001 bs=64 drop=0.1:  14%|█▍        | 14/100 [00:02<00:13,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.0005, 'batch_size': 64, 'dropout': 0.1}\n",
      "Params lr=0.0001, bs=64, drop=0.1: Best Val Acc = 0.8961\n",
      "Training with lr=0.0001, batch_size=64, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 lr=0.0001 bs=64 drop=0.2:  14%|█▍        | 14/100 [00:02<00:13,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.0005, 'batch_size': 64, 'dropout': 0.1}\n",
      "Params lr=0.0001, bs=64, drop=0.2: Best Val Acc = 0.8961\n",
      "Fold 2: Saved accuracy plot to plots_ViTHead/vithead_accuracy_fold_2.png\n",
      "\n",
      "Fold 2 Validation Accuracy: 0.8961\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.90      0.89      0.90       299\n",
      "       Dirty       0.89      0.90      0.90       298\n",
      "\n",
      "    accuracy                           0.90       597\n",
      "   macro avg       0.90      0.90      0.90       597\n",
      "weighted avg       0.90      0.90      0.90       597\n",
      "\n",
      "Confusion Matrix:\n",
      "[[266  33]\n",
      " [ 29 269]]\n",
      "\n",
      "=== Fold 3 ===\n",
      "Training with lr=0.001, batch_size=32, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 lr=0.001 bs=32 drop=0.1:  26%|██▌       | 26/100 [00:06<00:18,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 27 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.001, bs=32, drop=0.1: Best Val Acc = 0.9112\n",
      "Training with lr=0.001, batch_size=32, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 lr=0.001 bs=32 drop=0.2:  14%|█▍        | 14/100 [00:03<00:23,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.001, bs=32, drop=0.2: Best Val Acc = 0.9112\n",
      "Training with lr=0.001, batch_size=64, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 lr=0.001 bs=64 drop=0.1:  14%|█▍        | 14/100 [00:02<00:13,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.001, bs=64, drop=0.1: Best Val Acc = 0.9112\n",
      "Training with lr=0.001, batch_size=64, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 lr=0.001 bs=64 drop=0.2:  14%|█▍        | 14/100 [00:02<00:14,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.001, bs=64, drop=0.2: Best Val Acc = 0.9112\n",
      "Training with lr=0.0005, batch_size=32, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 lr=0.0005 bs=32 drop=0.1:  14%|█▍        | 14/100 [00:03<00:21,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.0005, bs=32, drop=0.1: Best Val Acc = 0.9112\n",
      "Training with lr=0.0005, batch_size=32, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 lr=0.0005 bs=32 drop=0.2:  14%|█▍        | 14/100 [00:03<00:21,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.0005, bs=32, drop=0.2: Best Val Acc = 0.9112\n",
      "Training with lr=0.0005, batch_size=64, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 lr=0.0005 bs=64 drop=0.1:  14%|█▍        | 14/100 [00:02<00:13,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.0005, bs=64, drop=0.1: Best Val Acc = 0.9112\n",
      "Training with lr=0.0005, batch_size=64, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 lr=0.0005 bs=64 drop=0.2:  14%|█▍        | 14/100 [00:02<00:14,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.0005, bs=64, drop=0.2: Best Val Acc = 0.9112\n",
      "Training with lr=0.0001, batch_size=32, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 lr=0.0001 bs=32 drop=0.1:  14%|█▍        | 14/100 [00:03<00:23,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.0001, bs=32, drop=0.1: Best Val Acc = 0.9112\n",
      "Training with lr=0.0001, batch_size=32, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 lr=0.0001 bs=32 drop=0.2:  14%|█▍        | 14/100 [00:03<00:23,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.0001, bs=32, drop=0.2: Best Val Acc = 0.9112\n",
      "Training with lr=0.0001, batch_size=64, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 lr=0.0001 bs=64 drop=0.1:  14%|█▍        | 14/100 [00:02<00:13,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.0001, bs=64, drop=0.1: Best Val Acc = 0.9112\n",
      "Training with lr=0.0001, batch_size=64, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 lr=0.0001 bs=64 drop=0.2:  14%|█▍        | 14/100 [00:02<00:14,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.0001, bs=64, drop=0.2: Best Val Acc = 0.9112\n",
      "Fold 3: Saved accuracy plot to plots_ViTHead/vithead_accuracy_fold_3.png\n",
      "\n",
      "Fold 3 Validation Accuracy: 0.9112\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.91      0.91      0.91       299\n",
      "       Dirty       0.91      0.91      0.91       298\n",
      "\n",
      "    accuracy                           0.91       597\n",
      "   macro avg       0.91      0.91      0.91       597\n",
      "weighted avg       0.91      0.91      0.91       597\n",
      "\n",
      "Confusion Matrix:\n",
      "[[272  27]\n",
      " [ 26 272]]\n",
      "\n",
      "=== Fold 4 ===\n",
      "Training with lr=0.001, batch_size=32, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 lr=0.001 bs=32 drop=0.1:  23%|██▎       | 23/100 [00:05<00:19,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 24 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.001, bs=32, drop=0.1: Best Val Acc = 0.9146\n",
      "Training with lr=0.001, batch_size=32, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 lr=0.001 bs=32 drop=0.2:  14%|█▍        | 14/100 [00:03<00:22,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.001, bs=32, drop=0.2: Best Val Acc = 0.9146\n",
      "Training with lr=0.001, batch_size=64, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 lr=0.001 bs=64 drop=0.1:  29%|██▉       | 29/100 [00:04<00:12,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 30 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.1}\n",
      "Params lr=0.001, bs=64, drop=0.1: Best Val Acc = 0.9162\n",
      "Training with lr=0.001, batch_size=64, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 lr=0.001 bs=64 drop=0.2:  23%|██▎       | 23/100 [00:03<00:11,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 24 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.001, bs=64, drop=0.2: Best Val Acc = 0.9179\n",
      "Training with lr=0.0005, batch_size=32, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 lr=0.0005 bs=32 drop=0.1:  14%|█▍        | 14/100 [00:03<00:21,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.0005, bs=32, drop=0.1: Best Val Acc = 0.9179\n",
      "Training with lr=0.0005, batch_size=32, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 lr=0.0005 bs=32 drop=0.2:  14%|█▍        | 14/100 [00:03<00:23,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.0005, bs=32, drop=0.2: Best Val Acc = 0.9179\n",
      "Training with lr=0.0005, batch_size=64, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 lr=0.0005 bs=64 drop=0.1:  14%|█▍        | 14/100 [00:02<00:13,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.0005, bs=64, drop=0.1: Best Val Acc = 0.9179\n",
      "Training with lr=0.0005, batch_size=64, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 lr=0.0005 bs=64 drop=0.2:  14%|█▍        | 14/100 [00:02<00:14,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.0005, bs=64, drop=0.2: Best Val Acc = 0.9179\n",
      "Training with lr=0.0001, batch_size=32, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 lr=0.0001 bs=32 drop=0.1:  14%|█▍        | 14/100 [00:03<00:22,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.0001, bs=32, drop=0.1: Best Val Acc = 0.9179\n",
      "Training with lr=0.0001, batch_size=32, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 lr=0.0001 bs=32 drop=0.2:  14%|█▍        | 14/100 [00:03<00:21,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.0001, bs=32, drop=0.2: Best Val Acc = 0.9179\n",
      "Training with lr=0.0001, batch_size=64, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 lr=0.0001 bs=64 drop=0.1:  14%|█▍        | 14/100 [00:02<00:14,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.0001, bs=64, drop=0.1: Best Val Acc = 0.9179\n",
      "Training with lr=0.0001, batch_size=64, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 lr=0.0001 bs=64 drop=0.2:  14%|█▍        | 14/100 [00:02<00:13,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.0001, bs=64, drop=0.2: Best Val Acc = 0.9179\n",
      "Fold 4: Saved accuracy plot to plots_ViTHead/vithead_accuracy_fold_4.png\n",
      "\n",
      "Fold 4 Validation Accuracy: 0.9179\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.91      0.93      0.92       298\n",
      "       Dirty       0.93      0.90      0.92       299\n",
      "\n",
      "    accuracy                           0.92       597\n",
      "   macro avg       0.92      0.92      0.92       597\n",
      "weighted avg       0.92      0.92      0.92       597\n",
      "\n",
      "Confusion Matrix:\n",
      "[[278  20]\n",
      " [ 29 270]]\n",
      "\n",
      "=== Fold 5 ===\n",
      "Training with lr=0.001, batch_size=32, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 lr=0.001 bs=32 drop=0.1:  21%|██        | 21/100 [00:05<00:20,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 22 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.001, bs=32, drop=0.1: Best Val Acc = 0.9045\n",
      "Training with lr=0.001, batch_size=32, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 lr=0.001 bs=32 drop=0.2:  14%|█▍        | 14/100 [00:03<00:24,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.001, bs=32, drop=0.2: Best Val Acc = 0.9045\n",
      "Training with lr=0.001, batch_size=64, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 lr=0.001 bs=64 drop=0.1:  14%|█▍        | 14/100 [00:02<00:13,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 32, 'dropout': 0.1}\n",
      "Params lr=0.001, bs=64, drop=0.1: Best Val Acc = 0.9045\n",
      "Training with lr=0.001, batch_size=64, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 lr=0.001 bs=64 drop=0.2:  25%|██▌       | 25/100 [00:03<00:10,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 26 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.001, bs=64, drop=0.2: Best Val Acc = 0.9079\n",
      "Training with lr=0.0005, batch_size=32, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 lr=0.0005 bs=32 drop=0.1:  14%|█▍        | 14/100 [00:03<00:20,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.0005, bs=32, drop=0.1: Best Val Acc = 0.9079\n",
      "Training with lr=0.0005, batch_size=32, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 lr=0.0005 bs=32 drop=0.2:  14%|█▍        | 14/100 [00:03<00:21,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.001, 'batch_size': 64, 'dropout': 0.2}\n",
      "Params lr=0.0005, bs=32, drop=0.2: Best Val Acc = 0.9079\n",
      "Training with lr=0.0005, batch_size=64, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 lr=0.0005 bs=64 drop=0.1:  19%|█▉        | 19/100 [00:02<00:11,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 20 with params {'lr': 0.0005, 'batch_size': 64, 'dropout': 0.1}\n",
      "Params lr=0.0005, bs=64, drop=0.1: Best Val Acc = 0.9095\n",
      "Training with lr=0.0005, batch_size=64, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 lr=0.0005 bs=64 drop=0.2:  14%|█▍        | 14/100 [00:02<00:13,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.0005, 'batch_size': 64, 'dropout': 0.1}\n",
      "Params lr=0.0005, bs=64, drop=0.2: Best Val Acc = 0.9095\n",
      "Training with lr=0.0001, batch_size=32, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 lr=0.0001 bs=32 drop=0.1:  14%|█▍        | 14/100 [00:03<00:21,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.0005, 'batch_size': 64, 'dropout': 0.1}\n",
      "Params lr=0.0001, bs=32, drop=0.1: Best Val Acc = 0.9095\n",
      "Training with lr=0.0001, batch_size=32, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 lr=0.0001 bs=32 drop=0.2:  14%|█▍        | 14/100 [00:03<00:20,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.0005, 'batch_size': 64, 'dropout': 0.1}\n",
      "Params lr=0.0001, bs=32, drop=0.2: Best Val Acc = 0.9095\n",
      "Training with lr=0.0001, batch_size=64, dropout=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 lr=0.0001 bs=64 drop=0.1:  14%|█▍        | 14/100 [00:02<00:12,  6.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.0005, 'batch_size': 64, 'dropout': 0.1}\n",
      "Params lr=0.0001, bs=64, drop=0.1: Best Val Acc = 0.9095\n",
      "Training with lr=0.0001, batch_size=64, dropout=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 lr=0.0001 bs=64 drop=0.2:  14%|█▍        | 14/100 [00:01<00:10,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 15 with params {'lr': 0.0005, 'batch_size': 64, 'dropout': 0.1}\n",
      "Params lr=0.0001, bs=64, drop=0.2: Best Val Acc = 0.9095\n",
      "Fold 5: Saved accuracy plot to plots_ViTHead/vithead_accuracy_fold_5.png\n",
      "\n",
      "Fold 5 Validation Accuracy: 0.9095\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.92      0.90      0.91       298\n",
      "       Dirty       0.90      0.92      0.91       299\n",
      "\n",
      "    accuracy                           0.91       597\n",
      "   macro avg       0.91      0.91      0.91       597\n",
      "weighted avg       0.91      0.91      0.91       597\n",
      "\n",
      "Confusion Matrix:\n",
      "[[268  30]\n",
      " [ 24 275]]\n",
      "Saved combined accuracy plot to plots_ViTHead/vithead_combined_accuracy.png\n",
      "\n",
      "Final Best Accuracy (across all folds): 0.9298\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies: pip install numpy pandas scikit-learn matplotlib joblib tqdm torch transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "# Set Matplotlib backend to 'Agg' for non-interactive environments\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Define ViT classification head with deeper MLP\n",
    "class ViTHead(nn.Module):\n",
    "    def __init__(self, hidden_dim=1024, num_classes=2, hidden_size=256, dropout=0.1):\n",
    "        super(ViTHead, self).__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.head(x)\n",
    "\n",
    "# Load features and labels\n",
    "X_resampled = np.load('X_resampled.npy')  # Shape: (3000, 1024) for ViT features\n",
    "y_resampled = np.load('y_resampled.npy')  # Shape: (3000,) with 0=Clean, 1=Dirty\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_resampled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "# Create output directories\n",
    "#os.makedirs('saved_models_ViTHead', exist_ok=True)\n",
    "os.makedirs('PLOTS_ViTHead_SVM&MLP', exist_ok=True)\n",
    "\n",
    "# 5-Fold Stratified Cross-Validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "all_train_acc = []\n",
    "all_val_acc = []\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-3, 5e-4, 1e-4],\n",
    "    'batch_size': [32, 64],\n",
    "    'dropout': [0.1, 0.2]\n",
    "}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_resampled, y_resampled)):\n",
    "    print(f\"\\n=== Fold {fold + 1} ===\")\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    best_model_state = None\n",
    "    best_val_preds = None\n",
    "    best_train_acc_per_epoch = []\n",
    "    best_val_acc_per_epoch = []\n",
    "    best_params = None\n",
    "    \n",
    "    # Hyperparameter tuning\n",
    "    for lr, bs, dropout in product(param_grid['learning_rate'], param_grid['batch_size'], param_grid['dropout']):\n",
    "        print(f\"Training with lr={lr}, batch_size={bs}, dropout={dropout}\")\n",
    "        \n",
    "        # Prepare data\n",
    "        X_train, X_val = X_resampled[train_idx], X_resampled[val_idx]\n",
    "        y_train, y_val = y_resampled[train_idx], y_resampled[val_idx]\n",
    "        \n",
    "        # Oversample Dirty class (optional, enable if Dirty F1-score < 0.95)\n",
    "        # dirty_idx = np.where(y_train == 1)[0]\n",
    "        # oversample_idx = np.random.choice(dirty_idx, size=len(dirty_idx), replace=True)\n",
    "        # X_train = np.vstack((X_train, X_train[oversample_idx]))\n",
    "        # y_train = np.hstack((y_train, y_train[oversample_idx]))\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "        \n",
    "        # Create DataLoaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=bs)\n",
    "        \n",
    "        # Initialize model, loss, optimizer, and scheduler\n",
    "        model = ViTHead(hidden_dim=1024, num_classes=2, hidden_size=256, dropout=dropout).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "        \n",
    "        # Training loop with early stopping\n",
    "        no_improve_count = 0\n",
    "        train_acc_per_epoch = []\n",
    "        val_acc_per_epoch = []\n",
    "        patience = 20\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        for epoch in tqdm(range(100), desc=f'Fold {fold + 1} lr={lr} bs={bs} drop={dropout}'):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += y_batch.size(0)\n",
    "                train_correct += (predicted == y_batch).sum().item()\n",
    "            train_acc = train_correct / train_total\n",
    "            train_acc_per_epoch.append(train_acc)\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            val_preds = []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    outputs = model(X_batch)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += y_batch.size(0)\n",
    "                    val_correct += (predicted == y_batch).sum().item()\n",
    "                    val_preds.extend(predicted.cpu().numpy())\n",
    "            val_acc = val_correct / val_total\n",
    "            val_acc_per_epoch.append(val_acc)\n",
    "            \n",
    "            # Scheduler step\n",
    "            scheduler.step(val_acc)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model_state = model.state_dict()\n",
    "                best_val_preds = val_preds\n",
    "                best_train_acc_per_epoch = train_acc_per_epoch.copy()\n",
    "                best_val_acc_per_epoch = val_acc_per_epoch.copy()\n",
    "                best_params = {'lr': lr, 'batch_size': bs, 'dropout': dropout}\n",
    "                no_improve_count = 0\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "            \n",
    "            if no_improve_count >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} with params {best_params}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"Params lr={lr}, bs={bs}, drop={dropout}: Best Val Acc = {best_val_acc:.4f}\")\n",
    "    \n",
    "    # Store fold accuracy\n",
    "    fold_accuracies.append(best_val_acc)\n",
    "    all_train_acc.append(best_train_acc_per_epoch)\n",
    "    all_val_acc.append(best_val_acc_per_epoch)\n",
    "    \n",
    "    # Plot training and validation accuracy for this fold\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(best_train_acc_per_epoch) + 1), best_train_acc_per_epoch, label='Train Accuracy', color='blue')\n",
    "    plt.plot(range(1, len(best_val_acc_per_epoch) + 1), best_val_acc_per_epoch, label='Validation Accuracy', color='orange')\n",
    "    plt.title(f'Fold {fold + 1} - ViTHead Accuracy (Params: {best_params})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0.8, 1.0)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'PLOTS_ViTHead_SVM&MLP/vithead_accuracy_fold_{fold + 1}.png')\n",
    "    plt.close()\n",
    "    print(f\"Fold {fold + 1}: Saved accuracy plot to plots_ViTHead/vithead_accuracy_fold_{fold + 1}.png\")\n",
    "    \n",
    "    # Classification report and confusion matrix\n",
    "    print(f\"\\nFold {fold + 1} Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_val, best_val_preds, target_names=['Clean', 'Dirty']))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, best_val_preds))\n",
    "    \n",
    "    # Save the best model\n",
    "    #model_path = f\"saved_models_ViTHead/vithead_fold_{fold + 1}.pt\"\n",
    "    #torch.save(best_model_state, model_path)\n",
    "    #print(f\"Fold {fold + 1}: Saved ViTHead model to {model_path}\")\n",
    "\n",
    "# Combined accuracy plot for all folds\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "for fold in range(5):\n",
    "    plt.plot(range(1, len(all_train_acc[fold]) + 1), all_train_acc[fold], \n",
    "             label=f'Fold {fold + 1} Train', color=colors[fold], linestyle='--')\n",
    "    plt.plot(range(1, len(all_val_acc[fold]) + 1), all_val_acc[fold], \n",
    "             label=f'Fold {fold + 1} Val', color=colors[fold], linestyle='-')\n",
    "plt.title('ViTHead Accuracy Across All Folds')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.8, 1.0)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('PLOTS_ViTHead_SVM&MLP/vithead_combined_accuracy.png')\n",
    "plt.close()\n",
    "print(\"Saved combined accuracy plot to plots_ViTHead/vithead_combined_accuracy.png\")\n",
    "\n",
    "# Final summary\n",
    "final_best_accuracy = max(fold_accuracies)\n",
    "print(f\"\\nFinal Best Accuracy (across all folds): {final_best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c0659",
   "metadata": {},
   "source": [
    "#### XGBoost 1 Ensembled with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f629aba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5-fold cross-validation with XGBoost and simple parameter tuning...\n",
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:30] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:30] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:30] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:31] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Best parameters: {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.1, 'scale_pos_weight': 1.0}\n",
      "Fold 1: Train Accuracy: 0.9966, Validation Accuracy: 0.8863\n",
      "Fold 1: Saved accuracy plot to plots_XGB/xgb_accuracy_fold_1.png\n",
      "Fold 1 Best Validation Accuracy: 0.8863\n",
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.88      0.89      0.89       299\n",
      "       Dirty       0.89      0.88      0.89       299\n",
      "\n",
      "    accuracy                           0.89       598\n",
      "   macro avg       0.89      0.89      0.89       598\n",
      "weighted avg       0.89      0.89      0.89       598\n",
      "\n",
      "XGBoost Confusion Matrix:\n",
      "[[267  32]\n",
      " [ 36 263]]\n",
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:31] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: Best parameters: {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.05, 'scale_pos_weight': 1.0}\n",
      "Fold 2: Train Accuracy: 0.9971, Validation Accuracy: 0.8693\n",
      "Fold 2: Saved accuracy plot to plots_XGB/xgb_accuracy_fold_2.png\n",
      "Fold 2 Best Validation Accuracy: 0.8693\n",
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.88      0.85      0.87       299\n",
      "       Dirty       0.86      0.89      0.87       298\n",
      "\n",
      "    accuracy                           0.87       597\n",
      "   macro avg       0.87      0.87      0.87       597\n",
      "weighted avg       0.87      0.87      0.87       597\n",
      "\n",
      "XGBoost Confusion Matrix:\n",
      "[[255  44]\n",
      " [ 34 264]]\n",
      "\n",
      "=== Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: Best parameters: {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.05, 'scale_pos_weight': 1.2}\n",
      "Fold 3: Train Accuracy: 0.9979, Validation Accuracy: 0.8945\n",
      "Fold 3: Saved accuracy plot to plots_XGB/xgb_accuracy_fold_3.png\n",
      "Fold 3 Best Validation Accuracy: 0.8945\n",
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.91      0.87      0.89       299\n",
      "       Dirty       0.88      0.92      0.90       298\n",
      "\n",
      "    accuracy                           0.89       597\n",
      "   macro avg       0.90      0.89      0.89       597\n",
      "weighted avg       0.90      0.89      0.89       597\n",
      "\n",
      "XGBoost Confusion Matrix:\n",
      "[[261  38]\n",
      " [ 25 273]]\n",
      "\n",
      "=== Fold 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4: Best parameters: {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.05, 'scale_pos_weight': 1.0}\n",
      "Fold 4: Train Accuracy: 0.9971, Validation Accuracy: 0.8911\n",
      "Fold 4: Saved accuracy plot to plots_XGB/xgb_accuracy_fold_4.png\n",
      "Fold 4 Best Validation Accuracy: 0.8911\n",
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.91      0.87      0.89       298\n",
      "       Dirty       0.88      0.91      0.89       299\n",
      "\n",
      "    accuracy                           0.89       597\n",
      "   macro avg       0.89      0.89      0.89       597\n",
      "weighted avg       0.89      0.89      0.89       597\n",
      "\n",
      "XGBoost Confusion Matrix:\n",
      "[[260  38]\n",
      " [ 27 272]]\n",
      "\n",
      "=== Fold 5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:24:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5: Best parameters: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.1, 'scale_pos_weight': 1.2}\n",
      "Fold 5: Train Accuracy: 0.9979, Validation Accuracy: 0.8911\n",
      "Fold 5: Saved accuracy plot to plots_XGB/xgb_accuracy_fold_5.png\n",
      "Fold 5 Best Validation Accuracy: 0.8911\n",
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.92      0.86      0.89       298\n",
      "       Dirty       0.87      0.92      0.89       299\n",
      "\n",
      "    accuracy                           0.89       597\n",
      "   macro avg       0.89      0.89      0.89       597\n",
      "weighted avg       0.89      0.89      0.89       597\n",
      "\n",
      "XGBoost Confusion Matrix:\n",
      "[[256  42]\n",
      " [ 23 276]]\n",
      "Saved combined validation accuracy plot to plots_XGB/xgb_combined_validation_accuracy.png\n",
      "Completed 5-fold cross-validation and plot generation\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import joblib\n",
    "import os\n",
    "from itertools import product\n",
    "\n",
    "# Set Matplotlib backend to 'Agg' for non-interactive environments\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Load features and labels\n",
    "X_resampled = np.load('X_resampled.npy')\n",
    "y_resampled = np.load('y_resampled.npy')\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_resampled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "# Create output directories\n",
    "#os.makedirs('saved_models_XGB', exist_ok=True)\n",
    "os.makedirs('PLOTS_XGB_1', exist_ok=True)\n",
    "\n",
    "# Prepare 5-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define simple parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 6],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'scale_pos_weight': [1.0, 1.2]\n",
    "}\n",
    "\n",
    "print(\"Running 5-fold cross-validation with XGBoost and simple parameter tuning...\")\n",
    "\n",
    "# Lists to store validation accuracy curves for all folds\n",
    "all_val_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_resampled, y_resampled)):\n",
    "    X_train, X_val = X_resampled[train_idx], X_resampled[val_idx]\n",
    "    y_train, y_val = y_resampled[train_idx], y_resampled[val_idx]\n",
    "\n",
    "    print(f\"\\n=== Fold {fold + 1} ===\")\n",
    "\n",
    "    # Feature selection: Select top 100 features\n",
    "    selector = SelectKBest(f_classif, k=100)\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_val_selected = selector.transform(X_val)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Manual parameter tuning\n",
    "    for n_estimators, max_depth, learning_rate, scale_pos_weight in product(\n",
    "        param_grid['n_estimators'], param_grid['max_depth'], \n",
    "        param_grid['learning_rate'], param_grid['scale_pos_weight']\n",
    "    ):\n",
    "        params = {\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'learning_rate': learning_rate,\n",
    "            'scale_pos_weight': scale_pos_weight\n",
    "        }\n",
    "        xgb_model = XGBClassifier(\n",
    "            random_state=42, \n",
    "            use_label_encoder=False, \n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "        xgb_model.set_params(**params)\n",
    "        xgb_model.fit(X_train_selected, y_train)\n",
    "\n",
    "        # Evaluate\n",
    "        train_pred = xgb_model.predict(X_train_selected)\n",
    "        val_pred = xgb_model.predict(X_val_selected)\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # Track best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_params = params\n",
    "            best_model = xgb_model\n",
    "            best_val_pred = val_pred\n",
    "\n",
    "    all_val_accuracies.append(val_accuracies)\n",
    "\n",
    "    print(f\"Fold {fold + 1}: Best parameters: {best_params}\")\n",
    "    print(f\"Fold {fold + 1}: Train Accuracy: {train_acc:.4f}, Validation Accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "    # Plot accuracy per parameter trial for this fold\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy', color='blue')\n",
    "    plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy', color='orange')\n",
    "    plt.title(f'Fold {fold + 1} - Accuracy per Parameter Trial')\n",
    "    plt.xlabel('Parameter Trial')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'PLOTS_XGB_1/xgb_accuracy_fold_{fold + 1}.png')\n",
    "    plt.close()\n",
    "    print(f\"Fold {fold + 1}: Saved accuracy plot to plots_XGB/xgb_accuracy_fold_{fold + 1}.png\")\n",
    "\n",
    "    # Print classification report and confusion matrix\n",
    "    print(f\"Fold {fold + 1} Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    print(\"XGBoost Classification Report:\")\n",
    "    print(classification_report(y_val, best_val_pred, target_names=['Clean', 'Dirty']))\n",
    "    print(\"XGBoost Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, best_val_pred))\n",
    "\n",
    "    # Save the best model\n",
    "    #odel_path = f\"saved_models_XGB/best_xgb_fold_{fold + 1}.joblib\"\n",
    "    #joblib.dump(best_model, model_path)\n",
    "    #print(f\"Fold {fold + 1}: Saved best XGBoost model to {model_path}\")\n",
    "\n",
    "# Plot combined validation accuracy for all folds\n",
    "if all_val_accuracies:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = ['blue', 'orange', 'green', 'red', 'purple']\n",
    "    max_trials = max(len(val_acc) for val_acc in all_val_accuracies)\n",
    "    for fold, val_acc in enumerate(all_val_accuracies):\n",
    "        trials = range(1, len(val_acc) + 1)\n",
    "        plt.plot(trials, val_acc, label=f'Fold {fold + 1}', color=colors[fold])\n",
    "    plt.title('XGBoost Validation Accuracy Across All Folds')\n",
    "    plt.xlabel('Parameter Trial')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('PLOTS_XGB_1/xgb_combined_validation_accuracy.png')\n",
    "    plt.close()\n",
    "    print(\"Saved combined validation accuracy plot to plots_XGB/xgb_combined_validation_accuracy.png\")\n",
    "\n",
    "print(\"Completed 5-fold cross-validation and plot generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62c090f",
   "metadata": {},
   "source": [
    "#### XGBoost 2 with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ff372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5-fold cross-validation with XGBoost and RandomizedSearchCV...\n",
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Best parameters: {'classifier__subsample': 1.0, 'classifier__scale_pos_weight': 1.5, 'classifier__n_estimators': 500, 'classifier__max_depth': 5, 'classifier__learning_rate': 0.05, 'classifier__colsample_bytree': 1.0}\n",
      "Fold 1: Train Accuracy: 0.9966, Validation Accuracy: 0.8980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Parameter Trials:   0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:   2%|▏         | 1/50 [00:00<00:06,  7.91it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:   4%|▍         | 2/50 [00:00<00:10,  4.67it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:   6%|▌         | 3/50 [00:00<00:11,  4.02it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:   8%|▊         | 4/50 [00:00<00:09,  5.07it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  10%|█         | 5/50 [00:00<00:08,  5.22it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  12%|█▏        | 6/50 [00:01<00:10,  4.19it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  14%|█▍        | 7/50 [00:01<00:16,  2.63it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  16%|█▌        | 8/50 [00:03<00:24,  1.72it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  18%|█▊        | 9/50 [00:04<00:33,  1.22it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  20%|██        | 10/50 [00:06<00:50,  1.25s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  22%|██▏       | 11/50 [00:06<00:38,  1.02it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  24%|██▍       | 12/50 [00:07<00:34,  1.10it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  26%|██▌       | 13/50 [00:08<00:30,  1.22it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  28%|██▊       | 14/50 [00:08<00:24,  1.48it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  30%|███       | 15/50 [00:09<00:25,  1.35it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  32%|███▏      | 16/50 [00:10<00:26,  1.27it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:54] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  34%|███▍      | 17/50 [00:11<00:30,  1.07it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  36%|███▌      | 18/50 [00:12<00:27,  1.15it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  38%|███▊      | 19/50 [00:12<00:22,  1.39it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  40%|████      | 20/50 [00:14<00:28,  1.05it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:25:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  42%|████▏     | 21/50 [00:16<00:34,  1.20s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  44%|████▍     | 22/50 [00:16<00:30,  1.09s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  46%|████▌     | 23/50 [00:17<00:24,  1.11it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  48%|████▊     | 24/50 [00:17<00:21,  1.22it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  50%|█████     | 25/50 [00:18<00:17,  1.44it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  52%|█████▏    | 26/50 [00:19<00:18,  1.30it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  54%|█████▍    | 27/50 [00:21<00:27,  1.20s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  56%|█████▌    | 28/50 [00:22<00:25,  1.15s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  58%|█████▊    | 29/50 [00:22<00:19,  1.10it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  60%|██████    | 30/50 [00:23<00:13,  1.50it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  62%|██████▏   | 31/50 [00:23<00:11,  1.63it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  64%|██████▍   | 32/50 [00:25<00:17,  1.03it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  66%|██████▌   | 33/50 [00:26<00:16,  1.06it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  68%|██████▊   | 34/50 [00:26<00:12,  1.32it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  70%|███████   | 35/50 [00:29<00:20,  1.40s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  72%|███████▏  | 36/50 [00:30<00:19,  1.40s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  74%|███████▍  | 37/50 [00:32<00:17,  1.34s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  76%|███████▌  | 38/50 [00:32<00:13,  1.09s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  78%|███████▊  | 39/50 [00:33<00:13,  1.19s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:18] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  80%|████████  | 40/50 [00:34<00:09,  1.03it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:18] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  82%|████████▏ | 41/50 [00:36<00:10,  1.22s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:20] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  84%|████████▍ | 42/50 [00:36<00:08,  1.08s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  86%|████████▌ | 43/50 [00:37<00:05,  1.24it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  88%|████████▊ | 44/50 [00:37<00:03,  1.66it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  90%|█████████ | 45/50 [00:37<00:02,  1.88it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  92%|█████████▏| 46/50 [00:39<00:04,  1.04s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:24] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  94%|█████████▍| 47/50 [00:41<00:03,  1.11s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  96%|█████████▌| 48/50 [00:43<00:02,  1.35s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials:  98%|█████████▊| 49/50 [00:43<00:01,  1.13s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:26:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 1 Parameter Trials: 100%|██████████| 50/50 [00:45<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Saved accuracy plot to plots_XGB/xgb_accuracy_fold_1.png\n",
      "Fold 1 Best Validation Accuracy: 0.8980\n",
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.90      0.89      0.90       299\n",
      "       Dirty       0.89      0.91      0.90       299\n",
      "\n",
      "    accuracy                           0.90       598\n",
      "   macro avg       0.90      0.90      0.90       598\n",
      "weighted avg       0.90      0.90      0.90       598\n",
      "\n",
      "XGBoost Confusion Matrix:\n",
      "[[266  33]\n",
      " [ 28 271]]\n",
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:20] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: Best parameters: {'classifier__subsample': 0.9, 'classifier__scale_pos_weight': 1.2, 'classifier__n_estimators': 500, 'classifier__max_depth': 5, 'classifier__learning_rate': 0.1, 'classifier__colsample_bytree': 0.7}\n",
      "Fold 2: Train Accuracy: 0.9971, Validation Accuracy: 0.8710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 Parameter Trials:   0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:   2%|▏         | 1/50 [00:00<00:06,  7.48it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:   4%|▍         | 2/50 [00:00<00:10,  4.70it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:   6%|▌         | 3/50 [00:00<00:11,  4.01it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:   8%|▊         | 4/50 [00:00<00:09,  5.08it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  10%|█         | 5/50 [00:00<00:08,  5.19it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  12%|█▏        | 6/50 [00:01<00:10,  4.16it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  14%|█▍        | 7/50 [00:02<00:20,  2.09it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  16%|█▌        | 8/50 [00:03<00:27,  1.54it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:24] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  18%|█▊        | 9/50 [00:04<00:36,  1.11it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  20%|██        | 10/50 [00:06<00:50,  1.27s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  22%|██▏       | 11/50 [00:07<00:38,  1.02it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  24%|██▍       | 12/50 [00:07<00:34,  1.09it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  26%|██▌       | 13/50 [00:08<00:34,  1.07it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:30] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  28%|██▊       | 14/50 [00:09<00:29,  1.20it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:30] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  30%|███       | 15/50 [00:10<00:29,  1.17it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:31] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  32%|███▏      | 16/50 [00:11<00:29,  1.14it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  34%|███▍      | 17/50 [00:12<00:28,  1.14it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  36%|███▌      | 18/50 [00:12<00:23,  1.37it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  38%|███▊      | 19/50 [00:13<00:19,  1.58it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  40%|████      | 20/50 [00:14<00:25,  1.17it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  42%|████▏     | 21/50 [00:16<00:32,  1.14s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  44%|████▍     | 22/50 [00:17<00:29,  1.06s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  46%|████▌     | 23/50 [00:18<00:27,  1.03s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  48%|████▊     | 24/50 [00:18<00:25,  1.02it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  50%|█████     | 25/50 [00:19<00:20,  1.24it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  52%|█████▏    | 26/50 [00:19<00:17,  1.38it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  54%|█████▍    | 27/50 [00:21<00:21,  1.08it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  56%|█████▌    | 28/50 [00:22<00:22,  1.03s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  58%|█████▊    | 29/50 [00:23<00:19,  1.06it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  60%|██████    | 30/50 [00:23<00:14,  1.36it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  62%|██████▏   | 31/50 [00:24<00:14,  1.30it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  64%|██████▍   | 32/50 [00:25<00:18,  1.03s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  66%|██████▌   | 33/50 [00:26<00:14,  1.18it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  68%|██████▊   | 34/50 [00:26<00:10,  1.58it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  70%|███████   | 35/50 [00:30<00:22,  1.52s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  72%|███████▏  | 36/50 [00:31<00:20,  1.49s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  74%|███████▍  | 37/50 [00:32<00:15,  1.18s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  76%|███████▌  | 38/50 [00:32<00:10,  1.11it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  78%|███████▊  | 39/50 [00:33<00:11,  1.08s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:54] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  80%|████████  | 40/50 [00:34<00:08,  1.12it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  82%|████████▏ | 41/50 [00:35<00:10,  1.13s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  84%|████████▍ | 42/50 [00:36<00:08,  1.05s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  86%|████████▌ | 43/50 [00:37<00:05,  1.17it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  88%|████████▊ | 44/50 [00:37<00:03,  1.56it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  90%|█████████ | 45/50 [00:37<00:02,  1.78it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:27:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  92%|█████████▏| 46/50 [00:39<00:04,  1.00s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:28:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  94%|█████████▍| 47/50 [00:41<00:03,  1.16s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:28:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  96%|█████████▌| 48/50 [00:43<00:02,  1.39s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:28:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials:  98%|█████████▊| 49/50 [00:43<00:01,  1.15s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:28:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 2 Parameter Trials: 100%|██████████| 50/50 [00:45<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: Saved accuracy plot to plots_XGB/xgb_accuracy_fold_2.png\n",
      "Fold 2 Best Validation Accuracy: 0.8710\n",
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.89      0.84      0.87       299\n",
      "       Dirty       0.85      0.90      0.87       298\n",
      "\n",
      "    accuracy                           0.87       597\n",
      "   macro avg       0.87      0.87      0.87       597\n",
      "weighted avg       0.87      0.87      0.87       597\n",
      "\n",
      "XGBoost Confusion Matrix:\n",
      "[[252  47]\n",
      " [ 30 268]]\n",
      "\n",
      "=== Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:28:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: Best parameters: {'classifier__subsample': 1.0, 'classifier__scale_pos_weight': 1.5, 'classifier__n_estimators': 500, 'classifier__max_depth': 5, 'classifier__learning_rate': 0.05, 'classifier__colsample_bytree': 1.0}\n",
      "Fold 3: Train Accuracy: 0.9979, Validation Accuracy: 0.8894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 Parameter Trials:   0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:28:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:   2%|▏         | 1/50 [00:00<00:06,  7.77it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:28:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:   4%|▍         | 2/50 [00:00<00:10,  4.76it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:28:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:   6%|▌         | 3/50 [00:00<00:11,  4.10it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:   8%|▊         | 4/50 [00:00<00:09,  4.99it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  10%|█         | 5/50 [00:01<00:09,  4.95it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  12%|█▏        | 6/50 [00:01<00:16,  2.63it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  14%|█▍        | 7/50 [00:03<00:32,  1.32it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  16%|█▌        | 8/50 [00:04<00:35,  1.18it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  18%|█▊        | 9/50 [00:05<00:34,  1.21it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  20%|██        | 10/50 [00:06<00:44,  1.10s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  22%|██▏       | 11/50 [00:07<00:39,  1.02s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  24%|██▍       | 12/50 [00:09<00:47,  1.25s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  26%|██▌       | 13/50 [00:10<00:44,  1.19s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  28%|██▊       | 14/50 [00:10<00:33,  1.07it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  30%|███       | 15/50 [00:11<00:27,  1.30it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  32%|███▏      | 16/50 [00:11<00:22,  1.49it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  34%|███▍      | 17/50 [00:12<00:21,  1.57it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  36%|███▌      | 18/50 [00:12<00:18,  1.71it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  38%|███▊      | 19/50 [00:13<00:20,  1.53it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  40%|████      | 20/50 [00:15<00:35,  1.19s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  42%|████▏     | 21/50 [00:17<00:35,  1.21s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  44%|████▍     | 22/50 [00:17<00:27,  1.03it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  46%|████▌     | 23/50 [00:18<00:22,  1.20it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  48%|████▊     | 24/50 [00:18<00:20,  1.26it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:18] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  50%|█████     | 25/50 [00:19<00:20,  1.20it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  52%|█████▏    | 26/50 [00:20<00:22,  1.08it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:20] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  54%|█████▍    | 27/50 [00:22<00:29,  1.27s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  56%|█████▌    | 28/50 [00:23<00:24,  1.12s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  58%|█████▊    | 29/50 [00:24<00:18,  1.11it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  60%|██████    | 30/50 [00:24<00:13,  1.50it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  62%|██████▏   | 31/50 [00:24<00:11,  1.61it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:24] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  64%|██████▍   | 32/50 [00:27<00:21,  1.18s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:26] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  66%|██████▌   | 33/50 [00:28<00:18,  1.12s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  68%|██████▊   | 34/50 [00:28<00:13,  1.15it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  70%|███████   | 35/50 [00:31<00:20,  1.37s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:30] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  72%|███████▏  | 36/50 [00:32<00:19,  1.42s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  74%|███████▍  | 37/50 [00:33<00:17,  1.36s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  76%|███████▌  | 38/50 [00:34<00:13,  1.14s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  78%|███████▊  | 39/50 [00:36<00:14,  1.32s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  80%|████████  | 40/50 [00:36<00:10,  1.06s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  82%|████████▏ | 41/50 [00:38<00:11,  1.29s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  84%|████████▍ | 42/50 [00:39<00:09,  1.20s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  86%|████████▌ | 43/50 [00:39<00:06,  1.05it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  88%|████████▊ | 44/50 [00:39<00:04,  1.43it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  90%|█████████ | 45/50 [00:40<00:02,  1.68it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  92%|█████████▏| 46/50 [00:42<00:04,  1.01s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  94%|█████████▍| 47/50 [00:43<00:03,  1.13s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  96%|█████████▌| 48/50 [00:45<00:02,  1.40s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials:  98%|█████████▊| 49/50 [00:46<00:01,  1.15s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:29:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 3 Parameter Trials: 100%|██████████| 50/50 [00:47<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: Saved accuracy plot to plots_XGB/xgb_accuracy_fold_3.png\n",
      "Fold 3 Best Validation Accuracy: 0.8894\n",
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.91      0.87      0.89       299\n",
      "       Dirty       0.87      0.91      0.89       298\n",
      "\n",
      "    accuracy                           0.89       597\n",
      "   macro avg       0.89      0.89      0.89       597\n",
      "weighted avg       0.89      0.89      0.89       597\n",
      "\n",
      "XGBoost Confusion Matrix:\n",
      "[[260  39]\n",
      " [ 27 271]]\n",
      "\n",
      "=== Fold 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4: Best parameters: {'classifier__subsample': 1.0, 'classifier__scale_pos_weight': 1.0, 'classifier__n_estimators': 500, 'classifier__max_depth': 5, 'classifier__learning_rate': 0.1, 'classifier__colsample_bytree': 0.9}\n",
      "Fold 4: Train Accuracy: 0.9971, Validation Accuracy: 0.8911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 Parameter Trials:   0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:   2%|▏         | 1/50 [00:00<00:06,  7.19it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:   4%|▍         | 2/50 [00:00<00:10,  4.42it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:   6%|▌         | 3/50 [00:00<00:12,  3.80it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:   8%|▊         | 4/50 [00:00<00:09,  4.76it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  10%|█         | 5/50 [00:01<00:12,  3.72it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  12%|█▏        | 6/50 [00:01<00:18,  2.41it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  14%|█▍        | 7/50 [00:03<00:35,  1.22it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  16%|█▌        | 8/50 [00:04<00:30,  1.36it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  18%|█▊        | 9/50 [00:05<00:32,  1.27it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  20%|██        | 10/50 [00:07<00:57,  1.44s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  22%|██▏       | 11/50 [00:08<00:48,  1.25s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  24%|██▍       | 12/50 [00:10<00:48,  1.28s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  26%|██▌       | 13/50 [00:10<00:39,  1.07s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  28%|██▊       | 14/50 [00:10<00:29,  1.21it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  30%|███       | 15/50 [00:11<00:24,  1.41it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  32%|███▏      | 16/50 [00:12<00:23,  1.45it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  34%|███▍      | 17/50 [00:13<00:29,  1.11it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  36%|███▌      | 18/50 [00:14<00:28,  1.13it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  38%|███▊      | 19/50 [00:15<00:26,  1.15it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  40%|████      | 20/50 [00:16<00:31,  1.04s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  42%|████▏     | 21/50 [00:17<00:29,  1.02s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:30:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  44%|████▍     | 22/50 [00:18<00:27,  1.00it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  46%|████▌     | 23/50 [00:19<00:29,  1.08s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  48%|████▊     | 24/50 [00:21<00:31,  1.22s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  50%|█████     | 25/50 [00:21<00:26,  1.04s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  52%|█████▏    | 26/50 [00:22<00:21,  1.12it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  54%|█████▍    | 27/50 [00:23<00:22,  1.02it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  56%|█████▌    | 28/50 [00:25<00:24,  1.12s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  58%|█████▊    | 29/50 [00:25<00:21,  1.01s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  60%|██████    | 30/50 [00:26<00:15,  1.26it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  62%|██████▏   | 31/50 [00:27<00:17,  1.11it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  64%|██████▍   | 32/50 [00:28<00:19,  1.09s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  66%|██████▌   | 33/50 [00:29<00:15,  1.13it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  68%|██████▊   | 34/50 [00:29<00:11,  1.39it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  70%|███████   | 35/50 [00:33<00:24,  1.62s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  72%|███████▏  | 36/50 [00:33<00:18,  1.32s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  74%|███████▍  | 37/50 [00:34<00:14,  1.09s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  76%|███████▌  | 38/50 [00:35<00:11,  1.06it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  78%|███████▊  | 39/50 [00:37<00:15,  1.43s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  80%|████████  | 40/50 [00:37<00:10,  1.06s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  82%|████████▏ | 41/50 [00:38<00:09,  1.06s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:20] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  84%|████████▍ | 42/50 [00:39<00:08,  1.03s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  86%|████████▌ | 43/50 [00:40<00:05,  1.17it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  88%|████████▊ | 44/50 [00:40<00:04,  1.46it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  90%|█████████ | 45/50 [00:41<00:03,  1.37it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  92%|█████████▏| 46/50 [00:43<00:04,  1.13s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  94%|█████████▍| 47/50 [00:44<00:03,  1.20s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:26] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  96%|█████████▌| 48/50 [00:46<00:02,  1.46s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials:  98%|█████████▊| 49/50 [00:47<00:01,  1.25s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:31:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 4 Parameter Trials: 100%|██████████| 50/50 [00:49<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4: Saved accuracy plot to plots_XGB/xgb_accuracy_fold_4.png\n",
      "Fold 4 Best Validation Accuracy: 0.8911\n",
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.90      0.88      0.89       298\n",
      "       Dirty       0.88      0.91      0.89       299\n",
      "\n",
      "    accuracy                           0.89       597\n",
      "   macro avg       0.89      0.89      0.89       597\n",
      "weighted avg       0.89      0.89      0.89       597\n",
      "\n",
      "XGBoost Confusion Matrix:\n",
      "[[261  37]\n",
      " [ 28 271]]\n",
      "\n",
      "=== Fold 5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5: Best parameters: {'classifier__subsample': 0.9, 'classifier__scale_pos_weight': 1.5, 'classifier__n_estimators': 300, 'classifier__max_depth': 7, 'classifier__learning_rate': 0.3, 'classifier__colsample_bytree': 1.0}\n",
      "Fold 5: Train Accuracy: 0.9979, Validation Accuracy: 0.8978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 Parameter Trials:   0%|          | 0/50 [00:00<?, ?it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:   2%|▏         | 1/50 [00:00<00:06,  7.33it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:   4%|▍         | 2/50 [00:00<00:11,  4.33it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:   6%|▌         | 3/50 [00:00<00:12,  3.77it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:24] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:   8%|▊         | 4/50 [00:00<00:10,  4.56it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:24] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  10%|█         | 5/50 [00:01<00:09,  4.53it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:24] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  12%|█▏        | 6/50 [00:01<00:16,  2.65it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  14%|█▍        | 7/50 [00:03<00:33,  1.27it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:26] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  16%|█▌        | 8/50 [00:04<00:34,  1.21it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  18%|█▊        | 9/50 [00:05<00:33,  1.21it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  20%|██        | 10/50 [00:07<00:52,  1.30s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:30] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  22%|██▏       | 11/50 [00:08<00:45,  1.16s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:31] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  24%|██▍       | 12/50 [00:10<00:51,  1.35s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  26%|██▌       | 13/50 [00:10<00:43,  1.16s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  28%|██▊       | 14/50 [00:11<00:32,  1.11it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  30%|███       | 15/50 [00:11<00:26,  1.32it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  32%|███▏      | 16/50 [00:12<00:22,  1.51it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  34%|███▍      | 17/50 [00:13<00:27,  1.20it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  36%|███▌      | 18/50 [00:14<00:26,  1.22it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  38%|███▊      | 19/50 [00:15<00:27,  1.12it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  40%|████      | 20/50 [00:16<00:32,  1.08s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  42%|████▏     | 21/50 [00:17<00:29,  1.02s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  44%|████▍     | 22/50 [00:18<00:27,  1.04it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  46%|████▌     | 23/50 [00:19<00:28,  1.04s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  48%|████▊     | 24/50 [00:21<00:31,  1.22s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  50%|█████     | 25/50 [00:21<00:24,  1.02it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  52%|█████▏    | 26/50 [00:22<00:20,  1.18it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  54%|█████▍    | 27/50 [00:23<00:21,  1.06it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  56%|█████▌    | 28/50 [00:24<00:23,  1.06s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  58%|█████▊    | 29/50 [00:25<00:20,  1.03it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  60%|██████    | 30/50 [00:25<00:15,  1.32it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  62%|██████▏   | 31/50 [00:26<00:16,  1.14it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  64%|██████▍   | 32/50 [00:28<00:18,  1.02s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  66%|██████▌   | 33/50 [00:28<00:14,  1.20it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  68%|██████▊   | 34/50 [00:28<00:09,  1.60it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  70%|███████   | 35/50 [00:32<00:25,  1.70s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  72%|███████▏  | 36/50 [00:33<00:19,  1.39s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  74%|███████▍  | 37/50 [00:34<00:14,  1.12s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  76%|███████▌  | 38/50 [00:34<00:10,  1.16it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:32:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  78%|███████▊  | 39/50 [00:37<00:15,  1.45s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:33:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  80%|████████  | 40/50 [00:37<00:11,  1.15s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:33:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  82%|████████▏ | 41/50 [00:38<00:10,  1.17s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:33:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  84%|████████▍ | 42/50 [00:39<00:07,  1.05it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:33:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  86%|████████▌ | 43/50 [00:39<00:05,  1.38it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:33:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  88%|████████▊ | 44/50 [00:39<00:03,  1.83it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:33:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  90%|█████████ | 45/50 [00:40<00:02,  1.81it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:33:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  92%|█████████▏| 46/50 [00:43<00:05,  1.28s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:33:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  94%|█████████▍| 47/50 [00:44<00:03,  1.19s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:33:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  96%|█████████▌| 48/50 [00:45<00:02,  1.13s/it]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:33:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials:  98%|█████████▊| 49/50 [00:45<00:00,  1.01it/s]c:\\Users\\user\\anaconda3\\envs\\xyz_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:33:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fold 5 Parameter Trials: 100%|██████████| 50/50 [00:48<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5: Saved accuracy plot to plots_XGB/xgb_accuracy_fold_5.png\n",
      "Fold 5 Best Validation Accuracy: 0.8978\n",
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.92      0.87      0.89       298\n",
      "       Dirty       0.87      0.93      0.90       299\n",
      "\n",
      "    accuracy                           0.90       597\n",
      "   macro avg       0.90      0.90      0.90       597\n",
      "weighted avg       0.90      0.90      0.90       597\n",
      "\n",
      "XGBoost Confusion Matrix:\n",
      "[[258  40]\n",
      " [ 21 278]]\n",
      "Saved combined validation accuracy plot to plots_XGB/xgb_combined_validation_accuracy.png\n",
      "Completed 5-fold cross-validation and plot generation\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import joblib\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Set Matplotlib backend to 'Agg' for non-interactive environments\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Load features and labels\n",
    "X_resampled = np.load('X_resampled.npy')\n",
    "y_resampled = np.load('y_resampled.npy')\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_resampled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "# Create output directories\n",
    "#os.makedirs('saved_models_XGB', exist_ok=True)\n",
    "os.makedirs('PLOTS_XGB_2', exist_ok=True)\n",
    "\n",
    "# Prepare 5-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define parameter distribution for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'classifier__n_estimators': [100, 200, 300, 500],\n",
    "    'classifier__max_depth': [3, 5, 7, 10],\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1, 0.3],\n",
    "    'classifier__subsample': [0.5, 0.7, 0.9, 1.0],\n",
    "    'classifier__colsample_bytree': [0.5, 0.7, 0.9, 1.0],\n",
    "    'classifier__scale_pos_weight': [1.0, 1.2, 1.5]\n",
    "}\n",
    "\n",
    "print(\"Running 5-fold cross-validation with XGBoost and RandomizedSearchCV...\")\n",
    "\n",
    "# Lists to store validation accuracy curves for all folds\n",
    "all_val_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_resampled, y_resampled)):\n",
    "    X_train, X_val = X_resampled[train_idx], X_resampled[val_idx]\n",
    "    y_train, y_val = y_resampled[train_idx], y_resampled[val_idx]\n",
    "\n",
    "    print(f\"\\n=== Fold {fold + 1} ===\")\n",
    "\n",
    "    # Create pipeline with feature selection and XGBoost\n",
    "    pipeline = Pipeline([\n",
    "        ('selector', SelectKBest(f_classif, k=100)),\n",
    "        ('classifier', XGBClassifier(random_state=42, use_label_encoder=False))\n",
    "    ])\n",
    "\n",
    "    # Initialize RandomizedSearchCV\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=50,  # Number of parameter combinations to try\n",
    "        cv=3,  # Inner CV for hyperparameter tuning\n",
    "        scoring='accuracy',  # Change to 'f1' for Dirty class focus\n",
    "        n_jobs=-1,  # Use all available CPU cores\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Train with RandomizedSearchCV\n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_pipeline = random_search.best_estimator_\n",
    "    print(f\"Fold {fold + 1}: Best parameters: {random_search.best_params_}\")\n",
    "\n",
    "    # Evaluate on training and validation sets\n",
    "    train_pred = best_pipeline.predict(X_train)\n",
    "    val_pred = best_pipeline.predict(X_val)\n",
    "    train_acc = accuracy_score(y_train, train_pred)\n",
    "    val_acc = accuracy_score(y_val, val_pred)\n",
    "    print(f\"Fold {fold + 1}: Train Accuracy: {train_acc:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # Collect accuracy for each parameter combination tested\n",
    "    param_combinations = random_search.cv_results_['params']\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for params in tqdm(param_combinations, desc=f\"Fold {fold + 1} Parameter Trials\"):\n",
    "        # Update pipeline parameters\n",
    "        pipeline.set_params(**{key: value for key, value in params.items()})\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        train_accuracies.append(accuracy_score(y_train, pipeline.predict(X_train)))\n",
    "        val_accuracies.append(accuracy_score(y_val, pipeline.predict(X_val)))\n",
    "\n",
    "    all_val_accuracies.append(val_accuracies)\n",
    "\n",
    "    # Plot accuracy per parameter trial for this fold\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy', color='blue')\n",
    "    plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy', color='orange')\n",
    "    plt.title(f'Fold {fold + 1} - Accuracy per Parameter Trial')\n",
    "    plt.xlabel('Parameter Trial')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'PLOTS_XGB_2/xgb_accuracy_fold_{fold + 1}.png')\n",
    "    plt.close()\n",
    "    print(f\"Fold {fold + 1}: Saved accuracy plot to plots_XGB/xgb_accuracy_fold_{fold + 1}.png\")\n",
    "\n",
    "    # Print classification report and confusion matrix\n",
    "    print(f\"Fold {fold + 1} Best Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(\"XGBoost Classification Report:\")\n",
    "    print(classification_report(y_val, val_pred, target_names=['Clean', 'Dirty']))\n",
    "    print(\"XGBoost Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, val_pred))\n",
    "\n",
    "    # Save the best model\n",
    "    #model_path = f\"saved_models_XGB/best_xgb_fold_{fold + 1}.joblib\"\n",
    "    #joblib.dump(best_pipeline, model_path)\n",
    "    #print(f\"Fold {fold + 1}: Saved best XGBoost model to {model_path}\")\n",
    "\n",
    "# Plot combined validation accuracy for all folds\n",
    "if all_val_accuracies:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = ['blue', 'orange', 'green', 'red', 'purple']\n",
    "    max_trials = max(len(val_acc) for val_acc in all_val_accuracies)\n",
    "    for fold, val_acc in enumerate(all_val_accuracies):\n",
    "        trials = range(1, len(val_acc) + 1)\n",
    "        plt.plot(trials, val_acc, label=f'Fold {fold + 1}', color=colors[fold])\n",
    "    plt.title('XGBoost Validation Accuracy Across All Folds')\n",
    "    plt.xlabel('Parameter Trial')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('PLOTS_XGB_2/xgb_combined_validation_accuracy.png')\n",
    "    plt.close()\n",
    "    print(\"Saved combined validation accuracy plot to plots_XGB/xgb_combined_validation_accuracy.png\")\n",
    "\n",
    "print(\"Completed 5-fold cross-validation and plot generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de10e45",
   "metadata": {},
   "source": [
    "#### SVM 1 with Best Accuracy without GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06547b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n",
      "Running 5-fold cross-validation with 100 epochs per fold and early stopping...\n",
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epochs:   1%|          | 1/100 [00:01<02:58,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc = 0.9389, Val Acc = 0.8846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epochs:   2%|▏         | 2/100 [00:03<02:31,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Acc = 0.9389, Val Acc = 0.8846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epochs:   3%|▎         | 3/100 [00:04<02:22,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Acc = 0.9389, Val Acc = 0.8846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epochs:   4%|▍         | 4/100 [00:05<02:18,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Acc = 0.9389, Val Acc = 0.8846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epochs:   5%|▌         | 5/100 [00:07<02:14,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Acc = 0.9389, Val Acc = 0.8846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epochs:   6%|▌         | 6/100 [00:08<02:12,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc = 0.9389, Val Acc = 0.8846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epochs:   7%|▋         | 7/100 [00:10<02:11,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc = 0.9389, Val Acc = 0.8846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epochs:   8%|▊         | 8/100 [00:11<02:08,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Acc = 0.9389, Val Acc = 0.8846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epochs:   9%|▉         | 9/100 [00:12<02:07,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Acc = 0.9389, Val Acc = 0.8846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epochs:  10%|█         | 10/100 [00:14<02:06,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc = 0.9389, Val Acc = 0.8846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epochs:  10%|█         | 10/100 [00:15<02:21,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Acc = 0.9389, Val Acc = 0.8846\n",
      "Early stopping at epoch 11 in fold 1\n",
      "\n",
      "Fold 1 SVM Best Validation Accuracy: 0.8846\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88       299\n",
      "           1       0.89      0.88      0.88       299\n",
      "\n",
      "    accuracy                           0.88       598\n",
      "   macro avg       0.88      0.88      0.88       598\n",
      "weighted avg       0.88      0.88      0.88       598\n",
      "\n",
      "SVM Confusion Matrix:\n",
      "[[265  34]\n",
      " [ 35 264]]\n",
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 Epochs:   1%|          | 1/100 [00:01<02:13,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc = 0.9368, Val Acc = 0.8827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 Epochs:   2%|▏         | 2/100 [00:02<02:16,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Acc = 0.9368, Val Acc = 0.8827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 Epochs:   3%|▎         | 3/100 [00:04<02:14,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Acc = 0.9368, Val Acc = 0.8827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 Epochs:   4%|▍         | 4/100 [00:05<02:12,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Acc = 0.9368, Val Acc = 0.8827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 Epochs:   5%|▌         | 5/100 [00:06<02:10,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Acc = 0.9368, Val Acc = 0.8827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 Epochs:   6%|▌         | 6/100 [00:08<02:09,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc = 0.9368, Val Acc = 0.8827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 Epochs:   7%|▋         | 7/100 [00:09<02:08,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc = 0.9368, Val Acc = 0.8827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 Epochs:   8%|▊         | 8/100 [00:11<02:06,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Acc = 0.9368, Val Acc = 0.8827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 Epochs:   9%|▉         | 9/100 [00:12<02:05,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Acc = 0.9368, Val Acc = 0.8827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 Epochs:  10%|█         | 10/100 [00:13<02:04,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc = 0.9368, Val Acc = 0.8827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 Epochs:  10%|█         | 10/100 [00:15<02:16,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Acc = 0.9368, Val Acc = 0.8827\n",
      "Early stopping at epoch 11 in fold 2\n",
      "\n",
      "Fold 2 SVM Best Validation Accuracy: 0.8827\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.86      0.88       299\n",
      "           1       0.87      0.90      0.88       298\n",
      "\n",
      "    accuracy                           0.88       597\n",
      "   macro avg       0.88      0.88      0.88       597\n",
      "weighted avg       0.88      0.88      0.88       597\n",
      "\n",
      "SVM Confusion Matrix:\n",
      "[[258  41]\n",
      " [ 29 269]]\n",
      "\n",
      "=== Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 Epochs:   1%|          | 1/100 [00:01<02:17,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc = 0.9364, Val Acc = 0.8894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 Epochs:   2%|▏         | 2/100 [00:02<02:15,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Acc = 0.9364, Val Acc = 0.8894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 Epochs:   3%|▎         | 3/100 [00:04<02:13,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Acc = 0.9364, Val Acc = 0.8894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 Epochs:   4%|▍         | 4/100 [00:05<02:12,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Acc = 0.9364, Val Acc = 0.8894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 Epochs:   5%|▌         | 5/100 [00:06<02:10,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Acc = 0.9364, Val Acc = 0.8894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 Epochs:   6%|▌         | 6/100 [00:08<02:08,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc = 0.9364, Val Acc = 0.8894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 Epochs:   7%|▋         | 7/100 [00:09<02:06,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc = 0.9364, Val Acc = 0.8894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 Epochs:   8%|▊         | 8/100 [00:10<02:06,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Acc = 0.9364, Val Acc = 0.8894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 Epochs:   9%|▉         | 9/100 [00:12<02:03,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Acc = 0.9364, Val Acc = 0.8894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 Epochs:  10%|█         | 10/100 [00:13<02:03,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc = 0.9364, Val Acc = 0.8894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 Epochs:  10%|█         | 10/100 [00:15<02:15,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Acc = 0.9364, Val Acc = 0.8894\n",
      "Early stopping at epoch 11 in fold 3\n",
      "\n",
      "Fold 3 SVM Best Validation Accuracy: 0.8894\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89       299\n",
      "           1       0.87      0.91      0.89       298\n",
      "\n",
      "    accuracy                           0.89       597\n",
      "   macro avg       0.89      0.89      0.89       597\n",
      "weighted avg       0.89      0.89      0.89       597\n",
      "\n",
      "SVM Confusion Matrix:\n",
      "[[259  40]\n",
      " [ 26 272]]\n",
      "\n",
      "=== Fold 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 Epochs:   1%|          | 1/100 [00:01<02:18,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc = 0.9372, Val Acc = 0.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 Epochs:   2%|▏         | 2/100 [00:02<02:15,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Acc = 0.9372, Val Acc = 0.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 Epochs:   3%|▎         | 3/100 [00:04<02:13,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Acc = 0.9372, Val Acc = 0.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 Epochs:   4%|▍         | 4/100 [00:05<02:11,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Acc = 0.9372, Val Acc = 0.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 Epochs:   5%|▌         | 5/100 [00:06<02:10,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Acc = 0.9372, Val Acc = 0.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 Epochs:   6%|▌         | 6/100 [00:08<02:08,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc = 0.9372, Val Acc = 0.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 Epochs:   7%|▋         | 7/100 [00:09<02:08,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc = 0.9372, Val Acc = 0.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 Epochs:   8%|▊         | 8/100 [00:11<02:06,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Acc = 0.9372, Val Acc = 0.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 Epochs:   9%|▉         | 9/100 [00:12<02:05,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Acc = 0.9372, Val Acc = 0.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 Epochs:  10%|█         | 10/100 [00:13<02:04,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc = 0.9372, Val Acc = 0.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 Epochs:  10%|█         | 10/100 [00:15<02:16,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Acc = 0.9372, Val Acc = 0.8777\n",
      "Early stopping at epoch 11 in fold 4\n",
      "\n",
      "Fold 4 SVM Best Validation Accuracy: 0.8777\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.86      0.88       298\n",
      "           1       0.87      0.89      0.88       299\n",
      "\n",
      "    accuracy                           0.88       597\n",
      "   macro avg       0.88      0.88      0.88       597\n",
      "weighted avg       0.88      0.88      0.88       597\n",
      "\n",
      "SVM Confusion Matrix:\n",
      "[[257  41]\n",
      " [ 32 267]]\n",
      "\n",
      "=== Fold 5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 Epochs:   1%|          | 1/100 [00:01<02:19,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc = 0.9372, Val Acc = 0.8928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 Epochs:   2%|▏         | 2/100 [00:02<02:15,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Acc = 0.9372, Val Acc = 0.8928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 Epochs:   3%|▎         | 3/100 [00:04<02:17,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Acc = 0.9372, Val Acc = 0.8928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 Epochs:   4%|▍         | 4/100 [00:05<02:14,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Acc = 0.9372, Val Acc = 0.8928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 Epochs:   5%|▌         | 5/100 [00:06<02:12,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Acc = 0.9372, Val Acc = 0.8928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 Epochs:   6%|▌         | 6/100 [00:08<02:10,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc = 0.9372, Val Acc = 0.8928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 Epochs:   7%|▋         | 7/100 [00:09<02:08,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc = 0.9372, Val Acc = 0.8928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 Epochs:   8%|▊         | 8/100 [00:11<02:07,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Acc = 0.9372, Val Acc = 0.8928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 Epochs:   9%|▉         | 9/100 [00:12<02:06,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Acc = 0.9372, Val Acc = 0.8928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 Epochs:  10%|█         | 10/100 [00:13<02:05,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc = 0.9372, Val Acc = 0.8928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 Epochs:  10%|█         | 10/100 [00:15<02:17,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Acc = 0.9372, Val Acc = 0.8928\n",
      "Early stopping at epoch 11 in fold 5\n",
      "\n",
      "Fold 5 SVM Best Validation Accuracy: 0.8928\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89       298\n",
      "           1       0.89      0.90      0.89       299\n",
      "\n",
      "    accuracy                           0.89       597\n",
      "   macro avg       0.89      0.89      0.89       597\n",
      "weighted avg       0.89      0.89      0.89       597\n",
      "\n",
      "SVM Confusion Matrix:\n",
      "[[263  35]\n",
      " [ 29 270]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# Load features and labels\n",
    "X_resampled = np.load('X_resampled.npy')\n",
    "y_resampled = np.load('y_resampled.npy')\n",
    "\n",
    "# Prepare 5-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 20\n",
    "\n",
    "print(\"Running 5-fold cross-validation with 100 epochs per fold and early stopping...\")\n",
    "\n",
    "# Main loop for cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_resampled, y_resampled)):\n",
    "    X_train, X_val = X_resampled[train_idx], X_resampled[val_idx]\n",
    "    y_train, y_val = y_resampled[train_idx], y_resampled[val_idx]\n",
    "\n",
    "    print(f\"\\n=== Fold {fold +1} ===\")\n",
    "    best_val_acc = 0\n",
    "    best_val_pred = None\n",
    "    no_improve_count = 0\n",
    "\n",
    "# Epoch Training loop (Per Fold)\n",
    "    for epoch in tqdm(range(1, 101), desc=f'Fold {fold +1} Epochs'):\n",
    "        svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "        svm_model.fit(X_train, y_train)\n",
    "\n",
    "        train_pred = svm_model.predict(X_train)\n",
    "        val_pred = svm_model.predict(X_val)\n",
    "\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "\n",
    "# Accuracy and Early Stopping Check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_pred = val_pred\n",
    "            no_improve_count = 0\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Acc = {train_acc:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "\n",
    "        if no_improve_count >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch} in fold {fold +1}\")\n",
    "            break\n",
    "\n",
    "    # After early stopping or 100 epochs, show report and confusion matrix for best epoch\n",
    "    print(f\"\\nFold {fold +1} SVM Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    print(\"SVM Classification Report:\")\n",
    "    print(classification_report(y_val, best_val_pred))\n",
    "    print(\"SVM Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, best_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e2488",
   "metadata": {},
   "source": [
    "#### SVM 2 with GridSearch - Best Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c960aad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n",
      "Running 5-fold cross-validation with GridSearchCV...\n",
      "\n",
      "=== Fold 1 ===\n",
      "Best parameters for Fold 1: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Fold 1 Best Validation Accuracy: 0.8696\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.87       299\n",
      "           1       0.89      0.85      0.87       299\n",
      "\n",
      "    accuracy                           0.87       598\n",
      "   macro avg       0.87      0.87      0.87       598\n",
      "weighted avg       0.87      0.87      0.87       598\n",
      "\n",
      "SVM Confusion Matrix:\n",
      "[[266  33]\n",
      " [ 45 254]]\n",
      "\n",
      "=== Fold 2 ===\n",
      "Best parameters for Fold 2: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Fold 2 Best Validation Accuracy: 0.8660\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.87       299\n",
      "           1       0.87      0.86      0.86       298\n",
      "\n",
      "    accuracy                           0.87       597\n",
      "   macro avg       0.87      0.87      0.87       597\n",
      "weighted avg       0.87      0.87      0.87       597\n",
      "\n",
      "SVM Confusion Matrix:\n",
      "[[261  38]\n",
      " [ 42 256]]\n",
      "\n",
      "=== Fold 3 ===\n",
      "Best parameters for Fold 3: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "Fold 3 Best Validation Accuracy: 0.8760\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.88       299\n",
      "           1       0.87      0.88      0.88       298\n",
      "\n",
      "    accuracy                           0.88       597\n",
      "   macro avg       0.88      0.88      0.88       597\n",
      "weighted avg       0.88      0.88      0.88       597\n",
      "\n",
      "SVM Confusion Matrix:\n",
      "[[260  39]\n",
      " [ 35 263]]\n",
      "\n",
      "=== Fold 4 ===\n",
      "Best parameters for Fold 4: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Fold 4 Best Validation Accuracy: 0.8894\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89       298\n",
      "           1       0.89      0.89      0.89       299\n",
      "\n",
      "    accuracy                           0.89       597\n",
      "   macro avg       0.89      0.89      0.89       597\n",
      "weighted avg       0.89      0.89      0.89       597\n",
      "\n",
      "SVM Confusion Matrix:\n",
      "[[266  32]\n",
      " [ 34 265]]\n",
      "\n",
      "=== Fold 5 ===\n",
      "Best parameters for Fold 5: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Fold 5 Best Validation Accuracy: 0.8677\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86       298\n",
      "           1       0.85      0.89      0.87       299\n",
      "\n",
      "    accuracy                           0.87       597\n",
      "   macro avg       0.87      0.87      0.87       597\n",
      "weighted avg       0.87      0.87      0.87       597\n",
      "\n",
      "SVM Confusion Matrix:\n",
      "[[253  45]\n",
      " [ 34 265]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "#Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# Load features and labels\n",
    "X_resampled = np.load('X_resampled.npy')\n",
    "y_resampled = np.load('y_resampled.npy')\n",
    "\n",
    "# Prepare 5-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "print(\"Running 5-fold cross-validation with GridSearchCV...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_resampled, y_resampled)):\n",
    "    X_train, X_val = X_resampled[train_idx], X_resampled[val_idx]\n",
    "    y_train, y_val = y_resampled[train_idx], y_resampled[val_idx]\n",
    "\n",
    "    print(f\"\\n=== Fold {fold +1} ===\")\n",
    "\n",
    "    # Set up GridSearchCV with 3-fold inner cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=SVC(random_state=42),\n",
    "        param_grid=param_grid,\n",
    "        cv=3,\n",
    "        scoring='accuracy',\n",
    "        verbose=0,\n",
    "        n_jobs=-1  # Use all CPU cores\n",
    "    )\n",
    "\n",
    "    # Fit the grid search on training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best estimator\n",
    "    best_svm = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for Fold {fold +1}: {grid_search.best_params_}\")\n",
    "\n",
    "    # Evaluate on validation data\n",
    "    val_pred = best_svm.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, val_pred)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Fold {fold +1} Best Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(\"SVM Classification Report:\")\n",
    "    print(classification_report(y_val, val_pred))\n",
    "    print(\"SVM Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, val_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08929957",
   "metadata": {},
   "source": [
    "### MLP 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbf4277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5-fold cross-validation with MLP and simple parameter tuning...\n",
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 (128,) relu lr=0.001:  62%|██████▏   | 62/100 [00:00<00:00, 88.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 63 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (128,), relu, lr=0.001: Best Val Acc = 0.8829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 (128,) relu lr=0.01:   9%|▉         | 9/100 [00:00<00:01, 69.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (128,), relu, lr=0.01: Best Val Acc = 0.8829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 (128,) tanh lr=0.001:   9%|▉         | 9/100 [00:00<00:01, 66.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (128,), tanh, lr=0.001: Best Val Acc = 0.8829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 (128,) tanh lr=0.01:   9%|▉         | 9/100 [00:00<00:01, 67.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (128,), tanh, lr=0.01: Best Val Acc = 0.8829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 (256, 128) relu lr=0.001:   9%|▉         | 9/100 [00:00<00:02, 35.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (256, 128), relu, lr=0.001: Best Val Acc = 0.8829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 (256, 128) relu lr=0.01:   9%|▉         | 9/100 [00:00<00:02, 38.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (256, 128), relu, lr=0.01: Best Val Acc = 0.8829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 (256, 128) tanh lr=0.001:   9%|▉         | 9/100 [00:00<00:03, 30.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (256, 128), tanh, lr=0.001: Best Val Acc = 0.8829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 (256, 128) tanh lr=0.01:   9%|▉         | 9/100 [00:00<00:02, 31.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (256, 128), tanh, lr=0.01: Best Val Acc = 0.8829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 (512, 256, 128) relu lr=0.001:   9%|▉         | 9/100 [00:00<00:09,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (512, 256, 128), relu, lr=0.001: Best Val Acc = 0.8829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 (512, 256, 128) relu lr=0.01:   9%|▉         | 9/100 [00:00<00:08, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (512, 256, 128), relu, lr=0.01: Best Val Acc = 0.8829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 (512, 256, 128) tanh lr=0.001:   9%|▉         | 9/100 [00:00<00:09,  9.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (512, 256, 128), tanh, lr=0.001: Best Val Acc = 0.8829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 (512, 256, 128) tanh lr=0.01:   9%|▉         | 9/100 [00:00<00:10,  9.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (512, 256, 128), tanh, lr=0.01: Best Val Acc = 0.8829\n",
      "Fold 1: Saved accuracy plot to plots_MLP/mlp_accuracy_fold_1.png\n",
      "\n",
      "Fold 1 Best Validation Accuracy: 0.8829\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.87      0.90      0.88       299\n",
      "       Dirty       0.89      0.87      0.88       299\n",
      "\n",
      "    accuracy                           0.88       598\n",
      "   macro avg       0.88      0.88      0.88       598\n",
      "weighted avg       0.88      0.88      0.88       598\n",
      "\n",
      "Confusion Matrix:\n",
      "[[268  31]\n",
      " [ 39 260]]\n",
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 (128,) relu lr=0.001:  21%|██        | 21/100 [00:00<00:00, 83.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 22 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (128,), relu, lr=0.001: Best Val Acc = 0.8325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 (128,) relu lr=0.01:  19%|█▉        | 19/100 [00:00<00:00, 88.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 20 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.01}\n",
      "Params (128,), relu, lr=0.01: Best Val Acc = 0.8509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 (128,) tanh lr=0.001:   9%|▉         | 9/100 [00:00<00:01, 66.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.01}\n",
      "Params (128,), tanh, lr=0.001: Best Val Acc = 0.8509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 (128,) tanh lr=0.01:   9%|▉         | 9/100 [00:00<00:01, 67.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.01}\n",
      "Params (128,), tanh, lr=0.01: Best Val Acc = 0.8509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 (256, 128) relu lr=0.001:   9%|▉         | 9/100 [00:00<00:02, 34.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.01}\n",
      "Params (256, 128), relu, lr=0.001: Best Val Acc = 0.8509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 (256, 128) relu lr=0.01:   9%|▉         | 9/100 [00:00<00:02, 35.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.01}\n",
      "Params (256, 128), relu, lr=0.01: Best Val Acc = 0.8509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 (256, 128) tanh lr=0.001:   9%|▉         | 9/100 [00:00<00:02, 32.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.01}\n",
      "Params (256, 128), tanh, lr=0.001: Best Val Acc = 0.8509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 (256, 128) tanh lr=0.01:   9%|▉         | 9/100 [00:00<00:02, 30.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.01}\n",
      "Params (256, 128), tanh, lr=0.01: Best Val Acc = 0.8509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 (512, 256, 128) relu lr=0.001:   9%|▉         | 9/100 [00:00<00:09,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.01}\n",
      "Params (512, 256, 128), relu, lr=0.001: Best Val Acc = 0.8509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 (512, 256, 128) relu lr=0.01:   9%|▉         | 9/100 [00:00<00:09,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.01}\n",
      "Params (512, 256, 128), relu, lr=0.01: Best Val Acc = 0.8509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 (512, 256, 128) tanh lr=0.001:  17%|█▋        | 17/100 [00:01<00:08,  9.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 18 for {'hidden_layer_sizes': (512, 256, 128), 'activation': 'tanh', 'learning_rate_init': 0.001}\n",
      "Params (512, 256, 128), tanh, lr=0.001: Best Val Acc = 0.8593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 (512, 256, 128) tanh lr=0.01:   9%|▉         | 9/100 [00:01<00:10,  8.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (512, 256, 128), 'activation': 'tanh', 'learning_rate_init': 0.001}\n",
      "Params (512, 256, 128), tanh, lr=0.01: Best Val Acc = 0.8593\n",
      "Fold 2: Saved accuracy plot to plots_MLP/mlp_accuracy_fold_2.png\n",
      "\n",
      "Fold 2 Best Validation Accuracy: 0.8593\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.85      0.87      0.86       299\n",
      "       Dirty       0.87      0.85      0.86       298\n",
      "\n",
      "    accuracy                           0.86       597\n",
      "   macro avg       0.86      0.86      0.86       597\n",
      "weighted avg       0.86      0.86      0.86       597\n",
      "\n",
      "Confusion Matrix:\n",
      "[[260  39]\n",
      " [ 45 253]]\n",
      "\n",
      "=== Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 (128,) relu lr=0.001:  26%|██▌       | 26/100 [00:00<00:00, 77.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 27 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (128,), relu, lr=0.001: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 (128,) relu lr=0.01:   9%|▉         | 9/100 [00:00<00:01, 78.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (128,), relu, lr=0.01: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 (128,) tanh lr=0.001:   9%|▉         | 9/100 [00:00<00:01, 73.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (128,), tanh, lr=0.001: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 (128,) tanh lr=0.01:   9%|▉         | 9/100 [00:00<00:01, 57.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (128,), tanh, lr=0.01: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 (256, 128) relu lr=0.001:   9%|▉         | 9/100 [00:00<00:02, 34.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (256, 128), relu, lr=0.001: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 (256, 128) relu lr=0.01:   9%|▉         | 9/100 [00:00<00:02, 36.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (256, 128), relu, lr=0.01: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 (256, 128) tanh lr=0.001:   9%|▉         | 9/100 [00:00<00:02, 31.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (256, 128), tanh, lr=0.001: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 (256, 128) tanh lr=0.01:   9%|▉         | 9/100 [00:00<00:02, 31.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (256, 128), tanh, lr=0.01: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 (512, 256, 128) relu lr=0.001:   9%|▉         | 9/100 [00:00<00:09,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (512, 256, 128), relu, lr=0.001: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 (512, 256, 128) relu lr=0.01:   9%|▉         | 9/100 [00:00<00:08, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (512, 256, 128), relu, lr=0.01: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 (512, 256, 128) tanh lr=0.001:   9%|▉         | 9/100 [00:01<00:10,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (512, 256, 128), tanh, lr=0.001: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 (512, 256, 128) tanh lr=0.01:   9%|▉         | 9/100 [00:01<00:14,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (512, 256, 128), tanh, lr=0.01: Best Val Acc = 0.8727\n",
      "Fold 3: Saved accuracy plot to plots_MLP/mlp_accuracy_fold_3.png\n",
      "\n",
      "Fold 3 Best Validation Accuracy: 0.8727\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.89      0.85      0.87       299\n",
      "       Dirty       0.85      0.90      0.88       298\n",
      "\n",
      "    accuracy                           0.87       597\n",
      "   macro avg       0.87      0.87      0.87       597\n",
      "weighted avg       0.87      0.87      0.87       597\n",
      "\n",
      "Confusion Matrix:\n",
      "[[253  46]\n",
      " [ 30 268]]\n",
      "\n",
      "=== Fold 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 (128,) relu lr=0.001:  35%|███▌      | 35/100 [00:00<00:00, 80.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 36 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (128,), relu, lr=0.001: Best Val Acc = 0.8710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 (128,) relu lr=0.01:   9%|▉         | 9/100 [00:00<00:01, 67.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (128,), relu, lr=0.01: Best Val Acc = 0.8710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 (128,) tanh lr=0.001:   9%|▉         | 9/100 [00:00<00:01, 58.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (128,), tanh, lr=0.001: Best Val Acc = 0.8710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 (128,) tanh lr=0.01:   9%|▉         | 9/100 [00:00<00:01, 50.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (128,), tanh, lr=0.01: Best Val Acc = 0.8710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 (256, 128) relu lr=0.001:  16%|█▌        | 16/100 [00:00<00:03, 24.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 17 for {'hidden_layer_sizes': (256, 128), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (256, 128), relu, lr=0.001: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 (256, 128) relu lr=0.01:   9%|▉         | 9/100 [00:00<00:03, 25.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (256, 128), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (256, 128), relu, lr=0.01: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 (256, 128) tanh lr=0.001:   9%|▉         | 9/100 [00:00<00:04, 19.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (256, 128), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (256, 128), tanh, lr=0.001: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 (256, 128) tanh lr=0.01:   9%|▉         | 9/100 [00:00<00:05, 18.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (256, 128), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (256, 128), tanh, lr=0.01: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 (512, 256, 128) relu lr=0.001:   9%|▉         | 9/100 [00:01<00:12,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (256, 128), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (512, 256, 128), relu, lr=0.001: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 (512, 256, 128) relu lr=0.01:   9%|▉         | 9/100 [00:01<00:11,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (256, 128), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (512, 256, 128), relu, lr=0.01: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 (512, 256, 128) tanh lr=0.001:   9%|▉         | 9/100 [00:01<00:14,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (256, 128), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (512, 256, 128), tanh, lr=0.001: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 (512, 256, 128) tanh lr=0.01:   9%|▉         | 9/100 [00:01<00:14,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (256, 128), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (512, 256, 128), tanh, lr=0.01: Best Val Acc = 0.8727\n",
      "Fold 4: Saved accuracy plot to plots_MLP/mlp_accuracy_fold_4.png\n",
      "\n",
      "Fold 4 Best Validation Accuracy: 0.8727\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.87      0.87      0.87       298\n",
      "       Dirty       0.87      0.87      0.87       299\n",
      "\n",
      "    accuracy                           0.87       597\n",
      "   macro avg       0.87      0.87      0.87       597\n",
      "weighted avg       0.87      0.87      0.87       597\n",
      "\n",
      "Confusion Matrix:\n",
      "[[260  38]\n",
      " [ 38 261]]\n",
      "\n",
      "=== Fold 5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 (128,) relu lr=0.001:  41%|████      | 41/100 [00:00<00:00, 89.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 42 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (128,), relu, lr=0.001: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 (128,) relu lr=0.01:   9%|▉         | 9/100 [00:00<00:01, 84.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (128,), relu, lr=0.01: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 (128,) tanh lr=0.001:   9%|▉         | 9/100 [00:00<00:01, 80.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (128,), tanh, lr=0.001: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 (128,) tanh lr=0.01:   9%|▉         | 9/100 [00:00<00:01, 77.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (128,), tanh, lr=0.01: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 (256, 128) relu lr=0.001:   9%|▉         | 9/100 [00:00<00:02, 32.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (256, 128), relu, lr=0.001: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 (256, 128) relu lr=0.01:   9%|▉         | 9/100 [00:00<00:02, 35.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (256, 128), relu, lr=0.01: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 (256, 128) tanh lr=0.001:   9%|▉         | 9/100 [00:00<00:02, 31.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (256, 128), tanh, lr=0.001: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 (256, 128) tanh lr=0.01:   9%|▉         | 9/100 [00:00<00:02, 31.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (256, 128), tanh, lr=0.01: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 (512, 256, 128) relu lr=0.001:   9%|▉         | 9/100 [00:00<00:09,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (512, 256, 128), relu, lr=0.001: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 (512, 256, 128) relu lr=0.01:   9%|▉         | 9/100 [00:00<00:09, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (512, 256, 128), relu, lr=0.01: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 (512, 256, 128) tanh lr=0.001:   9%|▉         | 9/100 [00:00<00:10,  9.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (512, 256, 128), tanh, lr=0.001: Best Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 (512, 256, 128) tanh lr=0.01:   9%|▉         | 9/100 [00:00<00:10,  9.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10 for {'hidden_layer_sizes': (128,), 'activation': 'relu', 'learning_rate_init': 0.001}\n",
      "Params (512, 256, 128), tanh, lr=0.01: Best Val Acc = 0.8727\n",
      "Fold 5: Saved accuracy plot to plots_MLP/mlp_accuracy_fold_5.png\n",
      "\n",
      "Fold 5 Best Validation Accuracy: 0.8727\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.89      0.85      0.87       298\n",
      "       Dirty       0.86      0.90      0.88       299\n",
      "\n",
      "    accuracy                           0.87       597\n",
      "   macro avg       0.87      0.87      0.87       597\n",
      "weighted avg       0.87      0.87      0.87       597\n",
      "\n",
      "Confusion Matrix:\n",
      "[[253  45]\n",
      " [ 31 268]]\n",
      "Saved combined validation accuracy plot to plots_MLP/mlp_combined_validation_accuracy.png\n",
      "\n",
      "Final Best Accuracy (across all folds): 0.8829\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import joblib\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from copy import deepcopy\n",
    "\n",
    "# Set Matplotlib backend to 'Agg' for non-interactive environments\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Load features and labels\n",
    "X_resampled = np.load('X_resampled.npy')\n",
    "y_resampled = np.load('y_resampled.npy')\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_resampled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "# Create output directories\n",
    "#os.makedirs('saved_models_MLP', exist_ok=True)\n",
    "os.makedirs('PLOTS_MLP_1', exist_ok=True)\n",
    "\n",
    "# Define MLP model creation function\n",
    "def create_mlp(hidden_layer_sizes=(256, 128), activation='relu', learning_rate_init=0.001, max_iter=1, warm_start=True):\n",
    "    return MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=activation,\n",
    "        learning_rate_init=learning_rate_init,\n",
    "        max_iter=max_iter,\n",
    "        solver='adam',\n",
    "        warm_start=warm_start,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(128,), (256, 128), (512, 256, 128)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "# 5-Fold Stratified Cross-Validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "\n",
    "patience = 20  # Early stopping patience\n",
    "max_epochs = 100\n",
    "\n",
    "print(\"Running 5-fold cross-validation with MLP and simple parameter tuning...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_resampled, y_resampled)):\n",
    "    print(f\"\\n=== Fold {fold + 1} ===\")\n",
    "    X_train, X_val = X_resampled[train_idx], X_resampled[val_idx]\n",
    "    y_train, y_val = y_resampled[train_idx], y_resampled[val_idx]\n",
    "\n",
    "    # Feature selection: Select top 100 features\n",
    "    selector = SelectKBest(f_classif, k=100)\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_val_selected = selector.transform(X_val)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    best_val_pred = None\n",
    "    best_train_acc_per_epoch = None\n",
    "    best_val_acc_per_epoch = None\n",
    "\n",
    "    # Manual parameter tuning\n",
    "    for hidden_layer_sizes, activation, learning_rate_init in product(\n",
    "        param_grid['hidden_layer_sizes'], param_grid['activation'], param_grid['learning_rate_init']\n",
    "    ):\n",
    "        mlp_model = create_mlp(\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            activation=activation,\n",
    "            learning_rate_init=learning_rate_init\n",
    "        )\n",
    "        no_improve_count = 0\n",
    "        train_acc_per_epoch = []\n",
    "        val_acc_per_epoch = []\n",
    "\n",
    "        for epoch in tqdm(range(1, max_epochs + 1), desc=f'Fold {fold + 1} {hidden_layer_sizes} {activation} lr={learning_rate_init}'):\n",
    "            mlp_model.partial_fit(X_train_selected, y_train, classes=np.unique(y_resampled))\n",
    "\n",
    "            y_train_pred = mlp_model.predict(X_train_selected)\n",
    "            y_val_pred = mlp_model.predict(X_val_selected)\n",
    "\n",
    "            train_acc = accuracy_score(y_train, y_train_pred)\n",
    "            val_acc = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "            train_acc_per_epoch.append(train_acc)\n",
    "            val_acc_per_epoch.append(val_acc)\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_params = {\n",
    "                    'hidden_layer_sizes': hidden_layer_sizes,\n",
    "                    'activation': activation,\n",
    "                    'learning_rate_init': learning_rate_init\n",
    "                }\n",
    "                best_model = deepcopy(mlp_model)\n",
    "                best_val_pred = y_val_pred\n",
    "                best_train_acc_per_epoch = train_acc_per_epoch.copy()\n",
    "                best_val_acc_per_epoch = val_acc_per_epoch.copy()\n",
    "                no_improve_count = 0\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "\n",
    "            if no_improve_count >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} for {best_params}\")\n",
    "                break\n",
    "\n",
    "        print(f\"Params {hidden_layer_sizes}, {activation}, lr={learning_rate_init}: Best Val Acc = {best_val_acc:.4f}\")\n",
    "\n",
    "    fold_accuracies.append(best_val_acc)\n",
    "\n",
    "    # Plot training and validation accuracy for the best model in this fold\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(best_train_acc_per_epoch) + 1), best_train_acc_per_epoch, label='Train Accuracy', color='blue')\n",
    "    plt.plot(range(1, len(best_val_acc_per_epoch) + 1), best_val_acc_per_epoch, label='Validation Accuracy', color='orange')\n",
    "    plt.title(f'Fold {fold + 1} - Best Model Accuracy (Params: {best_params})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0.8, 1.0)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'PLOTS_MLP_1/mlp_accuracy_fold_{fold + 1}.png')\n",
    "    plt.close()\n",
    "    print(f\"Fold {fold + 1}: Saved accuracy plot to plots_MLP/mlp_accuracy_fold_{fold + 1}.png\")\n",
    "\n",
    "    # Classification report and confusion matrix for best model\n",
    "    print(f\"\\nFold {fold + 1} Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_val, best_val_pred, target_names=['Clean', 'Dirty']))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, best_val_pred))\n",
    "\n",
    "    # Save the best model\n",
    "    #model_path = f\"saved_models_MLP/best_mlp_fold_{fold + 1}.joblib\"\n",
    "    #joblib.dump(best_model, model_path)\n",
    "    #print(f\"Fold {fold + 1}: Saved best MLP model to {model_path}\")\n",
    "\n",
    "# Plot combined validation accuracy for all folds\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['blue', 'orange', 'green', 'red', 'purple']\n",
    "for fold, val_acc in enumerate(fold_accuracies):\n",
    "    plt.plot(fold + 1, val_acc, 'o', color=colors[fold], label=f'Fold {fold + 1}')\n",
    "plt.title('MLP Best Validation Accuracy Across Folds')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.ylim(0.8, 1.0)\n",
    "plt.xticks(range(1, 6))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('PLOTS_MLP_1/mlp_combined_validation_accuracy.png')\n",
    "plt.close()\n",
    "print(\"Saved combined validation accuracy plot to plots_MLP/mlp_combined_validation_accuracy.png\")\n",
    "\n",
    "# Final summary\n",
    "final_best_accuracy = max(fold_accuracies)\n",
    "print(f\"\\nFinal Best Accuracy (across all folds): {final_best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51195d7a",
   "metadata": {},
   "source": [
    "### MLP 2 - Best One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c35260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cuda\n",
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epochs:  20%|██        | 14/70 [00:00<00:00, 133.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc = 0.4732, Val Acc = 0.7709\n",
      "Epoch 2: Train Acc = 0.7228, Val Acc = 0.7876\n",
      "Epoch 3: Train Acc = 0.7722, Val Acc = 0.7726\n",
      "Epoch 4: Train Acc = 0.7860, Val Acc = 0.7676\n",
      "Epoch 5: Train Acc = 0.7998, Val Acc = 0.7776\n",
      "Epoch 6: Train Acc = 0.8049, Val Acc = 0.7860\n",
      "Epoch 7: Train Acc = 0.8170, Val Acc = 0.7843\n",
      "Epoch 8: Train Acc = 0.8178, Val Acc = 0.7893\n",
      "Epoch 9: Train Acc = 0.8233, Val Acc = 0.7977\n",
      "Epoch 10: Train Acc = 0.8296, Val Acc = 0.8043\n",
      "Epoch 11: Train Acc = 0.8363, Val Acc = 0.8110\n",
      "Epoch 12: Train Acc = 0.8526, Val Acc = 0.8177\n",
      "Epoch 13: Train Acc = 0.8526, Val Acc = 0.8261\n",
      "Epoch 14: Train Acc = 0.8614, Val Acc = 0.8311\n",
      "Epoch 15: Train Acc = 0.8698, Val Acc = 0.8411\n",
      "Epoch 16: Train Acc = 0.8786, Val Acc = 0.8495\n",
      "Epoch 17: Train Acc = 0.8777, Val Acc = 0.8512\n",
      "Epoch 18: Train Acc = 0.8807, Val Acc = 0.8478\n",
      "Epoch 19: Train Acc = 0.8949, Val Acc = 0.8512\n",
      "Epoch 20: Train Acc = 0.8936, Val Acc = 0.8562\n",
      "Epoch 21: Train Acc = 0.8978, Val Acc = 0.8528\n",
      "Epoch 22: Train Acc = 0.9016, Val Acc = 0.8512\n",
      "Epoch 23: Train Acc = 0.9091, Val Acc = 0.8528\n",
      "Epoch 24: Train Acc = 0.9142, Val Acc = 0.8562\n",
      "Epoch 25: Train Acc = 0.9108, Val Acc = 0.8629\n",
      "Epoch 26: Train Acc = 0.9175, Val Acc = 0.8629\n",
      "Epoch 27: Train Acc = 0.9225, Val Acc = 0.8645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epochs:  64%|██████▍   | 45/70 [00:00<00:00, 131.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Acc = 0.9267, Val Acc = 0.8629\n",
      "Epoch 29: Train Acc = 0.9334, Val Acc = 0.8629\n",
      "Epoch 30: Train Acc = 0.9317, Val Acc = 0.8662\n",
      "Epoch 31: Train Acc = 0.9347, Val Acc = 0.8696\n",
      "Epoch 32: Train Acc = 0.9430, Val Acc = 0.8712\n",
      "Epoch 33: Train Acc = 0.9451, Val Acc = 0.8746\n",
      "Epoch 34: Train Acc = 0.9451, Val Acc = 0.8779\n",
      "Epoch 35: Train Acc = 0.9556, Val Acc = 0.8779\n",
      "Epoch 36: Train Acc = 0.9594, Val Acc = 0.8796\n",
      "Epoch 37: Train Acc = 0.9569, Val Acc = 0.8796\n",
      "Epoch 38: Train Acc = 0.9615, Val Acc = 0.8763\n",
      "Epoch 39: Train Acc = 0.9590, Val Acc = 0.8712\n",
      "Epoch 40: Train Acc = 0.9636, Val Acc = 0.8746\n",
      "Epoch 41: Train Acc = 0.9686, Val Acc = 0.8763\n",
      "Epoch 42: Train Acc = 0.9711, Val Acc = 0.8779\n",
      "Epoch 43: Train Acc = 0.9698, Val Acc = 0.8746\n",
      "Epoch 44: Train Acc = 0.9673, Val Acc = 0.8729\n",
      "Epoch 45: Train Acc = 0.9749, Val Acc = 0.8729\n",
      "Early stopping at epoch 46\n",
      "✅ Saved fold accuracy plot: plots_MLP2_593/mlp_accuracy_fold_1.png\n",
      "\n",
      "Fold 1 MLP Best Validation Accuracy: 0.8796\n",
      "MLP Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.88       296\n",
      "           1       0.86      0.91      0.88       302\n",
      "\n",
      "    accuracy                           0.88       598\n",
      "   macro avg       0.88      0.88      0.88       598\n",
      "weighted avg       0.88      0.88      0.88       598\n",
      "\n",
      "MLP Confusion Matrix:\n",
      "[[252  44]\n",
      " [ 28 274]]\n",
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 Epochs:  20%|██        | 14/70 [00:00<00:00, 139.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc = 0.5027, Val Acc = 0.7052\n",
      "Epoch 2: Train Acc = 0.6735, Val Acc = 0.7856\n",
      "Epoch 3: Train Acc = 0.7560, Val Acc = 0.7923\n",
      "Epoch 4: Train Acc = 0.7861, Val Acc = 0.7906\n",
      "Epoch 5: Train Acc = 0.8087, Val Acc = 0.7990\n",
      "Epoch 6: Train Acc = 0.8100, Val Acc = 0.7990\n",
      "Epoch 7: Train Acc = 0.8154, Val Acc = 0.8040\n",
      "Epoch 8: Train Acc = 0.8238, Val Acc = 0.8124\n",
      "Epoch 9: Train Acc = 0.8313, Val Acc = 0.8157\n",
      "Epoch 10: Train Acc = 0.8372, Val Acc = 0.8208\n",
      "Epoch 11: Train Acc = 0.8451, Val Acc = 0.8241\n",
      "Epoch 12: Train Acc = 0.8476, Val Acc = 0.8291\n",
      "Epoch 13: Train Acc = 0.8585, Val Acc = 0.8375\n",
      "Epoch 14: Train Acc = 0.8635, Val Acc = 0.8392\n",
      "Epoch 15: Train Acc = 0.8740, Val Acc = 0.8442\n",
      "Epoch 16: Train Acc = 0.8774, Val Acc = 0.8476\n",
      "Epoch 17: Train Acc = 0.8866, Val Acc = 0.8526\n",
      "Epoch 18: Train Acc = 0.8920, Val Acc = 0.8543\n",
      "Epoch 19: Train Acc = 0.8983, Val Acc = 0.8559\n",
      "Epoch 20: Train Acc = 0.9025, Val Acc = 0.8610\n",
      "Epoch 21: Train Acc = 0.9079, Val Acc = 0.8643\n",
      "Epoch 22: Train Acc = 0.9071, Val Acc = 0.8660\n",
      "Epoch 23: Train Acc = 0.9171, Val Acc = 0.8710\n",
      "Epoch 24: Train Acc = 0.9171, Val Acc = 0.8744\n",
      "Epoch 25: Train Acc = 0.9180, Val Acc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 Epochs:  41%|████▏     | 29/70 [00:00<00:00, 143.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Acc = 0.9242, Val Acc = 0.8794\n",
      "Epoch 27: Train Acc = 0.9343, Val Acc = 0.8777\n",
      "Epoch 28: Train Acc = 0.9360, Val Acc = 0.8744\n",
      "Epoch 29: Train Acc = 0.9364, Val Acc = 0.8760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 Epochs:  66%|██████▌   | 46/70 [00:00<00:00, 139.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Acc = 0.9385, Val Acc = 0.8777\n",
      "Epoch 31: Train Acc = 0.9468, Val Acc = 0.8794\n",
      "Epoch 32: Train Acc = 0.9464, Val Acc = 0.8811\n",
      "Epoch 33: Train Acc = 0.9439, Val Acc = 0.8827\n",
      "Epoch 34: Train Acc = 0.9531, Val Acc = 0.8844\n",
      "Epoch 35: Train Acc = 0.9586, Val Acc = 0.8894\n",
      "Epoch 36: Train Acc = 0.9581, Val Acc = 0.8928\n",
      "Epoch 37: Train Acc = 0.9607, Val Acc = 0.8945\n",
      "Epoch 38: Train Acc = 0.9632, Val Acc = 0.8928\n",
      "Epoch 39: Train Acc = 0.9724, Val Acc = 0.8878\n",
      "Epoch 40: Train Acc = 0.9690, Val Acc = 0.8861\n",
      "Epoch 41: Train Acc = 0.9740, Val Acc = 0.8811\n",
      "Epoch 42: Train Acc = 0.9732, Val Acc = 0.8811\n",
      "Epoch 43: Train Acc = 0.9711, Val Acc = 0.8827\n",
      "Epoch 44: Train Acc = 0.9740, Val Acc = 0.8811\n",
      "Epoch 45: Train Acc = 0.9740, Val Acc = 0.8878\n",
      "Epoch 46: Train Acc = 0.9795, Val Acc = 0.8861\n",
      "Early stopping at epoch 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fold accuracy plot: plots_MLP2_593/mlp_accuracy_fold_2.png\n",
      "\n",
      "Fold 2 MLP Best Validation Accuracy: 0.8945\n",
      "MLP Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90       312\n",
      "           1       0.87      0.91      0.89       285\n",
      "\n",
      "    accuracy                           0.89       597\n",
      "   macro avg       0.89      0.90      0.89       597\n",
      "weighted avg       0.90      0.89      0.89       597\n",
      "\n",
      "MLP Confusion Matrix:\n",
      "[[274  38]\n",
      " [ 25 260]]\n",
      "\n",
      "=== Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 Epochs:  20%|██        | 14/70 [00:00<00:00, 139.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc = 0.4768, Val Acc = 0.7119\n",
      "Epoch 2: Train Acc = 0.7112, Val Acc = 0.7755\n",
      "Epoch 3: Train Acc = 0.7627, Val Acc = 0.8023\n",
      "Epoch 4: Train Acc = 0.7853, Val Acc = 0.8057\n",
      "Epoch 5: Train Acc = 0.8028, Val Acc = 0.8174\n",
      "Epoch 6: Train Acc = 0.8137, Val Acc = 0.8208\n",
      "Epoch 7: Train Acc = 0.8183, Val Acc = 0.8308\n",
      "Epoch 8: Train Acc = 0.8208, Val Acc = 0.8291\n",
      "Epoch 9: Train Acc = 0.8288, Val Acc = 0.8258\n",
      "Epoch 10: Train Acc = 0.8388, Val Acc = 0.8308\n",
      "Epoch 11: Train Acc = 0.8405, Val Acc = 0.8375\n",
      "Epoch 12: Train Acc = 0.8460, Val Acc = 0.8409\n",
      "Epoch 13: Train Acc = 0.8531, Val Acc = 0.8342\n",
      "Epoch 14: Train Acc = 0.8627, Val Acc = 0.8375\n",
      "Epoch 15: Train Acc = 0.8740, Val Acc = 0.8375\n",
      "Epoch 16: Train Acc = 0.8803, Val Acc = 0.8509\n",
      "Epoch 17: Train Acc = 0.8891, Val Acc = 0.8593\n",
      "Epoch 18: Train Acc = 0.8861, Val Acc = 0.8610\n",
      "Epoch 19: Train Acc = 0.8882, Val Acc = 0.8610\n",
      "Epoch 20: Train Acc = 0.8987, Val Acc = 0.8693\n",
      "Epoch 21: Train Acc = 0.9012, Val Acc = 0.8677\n",
      "Epoch 22: Train Acc = 0.9067, Val Acc = 0.8643\n",
      "Epoch 23: Train Acc = 0.9159, Val Acc = 0.8660\n",
      "Epoch 24: Train Acc = 0.9134, Val Acc = 0.8677\n",
      "Epoch 25: Train Acc = 0.9188, Val Acc = 0.8643\n",
      "Epoch 26: Train Acc = 0.9276, Val Acc = 0.8660\n",
      "Epoch 27: Train Acc = 0.9263, Val Acc = 0.8677\n",
      "Epoch 28: Train Acc = 0.9301, Val Acc = 0.8710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 Epochs:  64%|██████▍   | 45/70 [00:00<00:00, 139.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Acc = 0.9410, Val Acc = 0.8710\n",
      "Epoch 30: Train Acc = 0.9401, Val Acc = 0.8727\n",
      "Epoch 31: Train Acc = 0.9464, Val Acc = 0.8710\n",
      "Epoch 32: Train Acc = 0.9481, Val Acc = 0.8710\n",
      "Epoch 33: Train Acc = 0.9506, Val Acc = 0.8744\n",
      "Epoch 34: Train Acc = 0.9560, Val Acc = 0.8727\n",
      "Epoch 35: Train Acc = 0.9556, Val Acc = 0.8727\n",
      "Epoch 36: Train Acc = 0.9573, Val Acc = 0.8727\n",
      "Epoch 37: Train Acc = 0.9632, Val Acc = 0.8677\n",
      "Epoch 38: Train Acc = 0.9615, Val Acc = 0.8677\n",
      "Epoch 39: Train Acc = 0.9611, Val Acc = 0.8727\n",
      "Epoch 40: Train Acc = 0.9653, Val Acc = 0.8727\n",
      "Epoch 41: Train Acc = 0.9732, Val Acc = 0.8777\n",
      "Epoch 42: Train Acc = 0.9699, Val Acc = 0.8760\n",
      "Epoch 43: Train Acc = 0.9720, Val Acc = 0.8727\n",
      "Epoch 44: Train Acc = 0.9736, Val Acc = 0.8744\n",
      "Epoch 45: Train Acc = 0.9753, Val Acc = 0.8777\n",
      "Epoch 46: Train Acc = 0.9778, Val Acc = 0.8760\n",
      "Epoch 47: Train Acc = 0.9795, Val Acc = 0.8794\n",
      "Epoch 48: Train Acc = 0.9824, Val Acc = 0.8811\n",
      "Epoch 49: Train Acc = 0.9795, Val Acc = 0.8827\n",
      "Epoch 50: Train Acc = 0.9837, Val Acc = 0.8844\n",
      "Epoch 51: Train Acc = 0.9849, Val Acc = 0.8861\n",
      "Epoch 52: Train Acc = 0.9849, Val Acc = 0.8811\n",
      "Epoch 53: Train Acc = 0.9841, Val Acc = 0.8794\n",
      "Epoch 54: Train Acc = 0.9879, Val Acc = 0.8794\n",
      "Epoch 55: Train Acc = 0.9874, Val Acc = 0.8794\n",
      "Epoch 56: Train Acc = 0.9887, Val Acc = 0.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 Epochs:  86%|████████▌ | 60/70 [00:00<00:00, 136.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: Train Acc = 0.9929, Val Acc = 0.8827\n",
      "Epoch 58: Train Acc = 0.9900, Val Acc = 0.8861\n",
      "Epoch 59: Train Acc = 0.9883, Val Acc = 0.8861\n",
      "Epoch 60: Train Acc = 0.9904, Val Acc = 0.8811\n",
      "Early stopping at epoch 61\n",
      "✅ Saved fold accuracy plot: plots_MLP2_593/mlp_accuracy_fold_3.png\n",
      "\n",
      "Fold 3 MLP Best Validation Accuracy: 0.8861\n",
      "MLP Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.90      0.88       284\n",
      "           1       0.90      0.88      0.89       313\n",
      "\n",
      "    accuracy                           0.89       597\n",
      "   macro avg       0.89      0.89      0.89       597\n",
      "weighted avg       0.89      0.89      0.89       597\n",
      "\n",
      "MLP Confusion Matrix:\n",
      "[[255  29]\n",
      " [ 39 274]]\n",
      "\n",
      "=== Fold 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 Epochs:   0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc = 0.4893, Val Acc = 0.7739\n",
      "Epoch 2: Train Acc = 0.7175, Val Acc = 0.8057\n",
      "Epoch 3: Train Acc = 0.7706, Val Acc = 0.8023\n",
      "Epoch 4: Train Acc = 0.7907, Val Acc = 0.8107\n",
      "Epoch 5: Train Acc = 0.7936, Val Acc = 0.8141\n",
      "Epoch 6: Train Acc = 0.8020, Val Acc = 0.8191\n",
      "Epoch 7: Train Acc = 0.8108, Val Acc = 0.8224\n",
      "Epoch 8: Train Acc = 0.8175, Val Acc = 0.8258\n",
      "Epoch 9: Train Acc = 0.8188, Val Acc = 0.8258\n",
      "Epoch 10: Train Acc = 0.8229, Val Acc = 0.8241\n",
      "Epoch 11: Train Acc = 0.8351, Val Acc = 0.8275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 Epochs:  37%|███▋      | 26/70 [00:00<00:00, 129.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Acc = 0.8393, Val Acc = 0.8325\n",
      "Epoch 13: Train Acc = 0.8481, Val Acc = 0.8392\n",
      "Epoch 14: Train Acc = 0.8506, Val Acc = 0.8459\n",
      "Epoch 15: Train Acc = 0.8589, Val Acc = 0.8509\n",
      "Epoch 16: Train Acc = 0.8702, Val Acc = 0.8509\n",
      "Epoch 17: Train Acc = 0.8761, Val Acc = 0.8526\n",
      "Epoch 18: Train Acc = 0.8761, Val Acc = 0.8593\n",
      "Epoch 19: Train Acc = 0.8907, Val Acc = 0.8660\n",
      "Epoch 20: Train Acc = 0.8861, Val Acc = 0.8660\n",
      "Epoch 21: Train Acc = 0.8928, Val Acc = 0.8710\n",
      "Epoch 22: Train Acc = 0.9012, Val Acc = 0.8677\n",
      "Epoch 23: Train Acc = 0.8987, Val Acc = 0.8677\n",
      "Epoch 24: Train Acc = 0.9104, Val Acc = 0.8677\n",
      "Epoch 25: Train Acc = 0.9154, Val Acc = 0.8660\n",
      "Epoch 26: Train Acc = 0.9196, Val Acc = 0.8693\n",
      "Epoch 27: Train Acc = 0.9267, Val Acc = 0.8693\n",
      "Epoch 28: Train Acc = 0.9255, Val Acc = 0.8727\n",
      "Epoch 29: Train Acc = 0.9297, Val Acc = 0.8727\n",
      "Epoch 30: Train Acc = 0.9318, Val Acc = 0.8710\n",
      "Epoch 31: Train Acc = 0.9334, Val Acc = 0.8710\n",
      "Epoch 32: Train Acc = 0.9406, Val Acc = 0.8727\n",
      "Epoch 33: Train Acc = 0.9443, Val Acc = 0.8727\n",
      "Epoch 34: Train Acc = 0.9464, Val Acc = 0.8693\n",
      "Epoch 35: Train Acc = 0.9514, Val Acc = 0.8744\n",
      "Epoch 36: Train Acc = 0.9464, Val Acc = 0.8727\n",
      "Epoch 37: Train Acc = 0.9577, Val Acc = 0.8760\n",
      "Epoch 38: Train Acc = 0.9577, Val Acc = 0.8744\n",
      "Epoch 39: Train Acc = 0.9627, Val Acc = 0.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 Epochs:  69%|██████▊   | 48/70 [00:00<00:00, 131.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: Train Acc = 0.9644, Val Acc = 0.8760\n",
      "Epoch 41: Train Acc = 0.9623, Val Acc = 0.8727\n",
      "Epoch 42: Train Acc = 0.9682, Val Acc = 0.8710\n",
      "Epoch 43: Train Acc = 0.9669, Val Acc = 0.8744\n",
      "Epoch 44: Train Acc = 0.9728, Val Acc = 0.8727\n",
      "Epoch 45: Train Acc = 0.9724, Val Acc = 0.8710\n",
      "Epoch 46: Train Acc = 0.9736, Val Acc = 0.8727\n",
      "Epoch 47: Train Acc = 0.9740, Val Acc = 0.8760\n",
      "Epoch 48: Train Acc = 0.9782, Val Acc = 0.8727\n",
      "Early stopping at epoch 49\n",
      "✅ Saved fold accuracy plot: plots_MLP2_593/mlp_accuracy_fold_4.png\n",
      "\n",
      "Fold 4 MLP Best Validation Accuracy: 0.8777\n",
      "MLP Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.87       287\n",
      "           1       0.88      0.89      0.88       310\n",
      "\n",
      "    accuracy                           0.88       597\n",
      "   macro avg       0.88      0.88      0.88       597\n",
      "weighted avg       0.88      0.88      0.88       597\n",
      "\n",
      "MLP Confusion Matrix:\n",
      "[[249  38]\n",
      " [ 35 275]]\n",
      "\n",
      "=== Fold 5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 Epochs:   0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc = 0.4914, Val Acc = 0.6817\n",
      "Epoch 2: Train Acc = 0.6660, Val Acc = 0.7722\n",
      "Epoch 3: Train Acc = 0.7614, Val Acc = 0.7856\n",
      "Epoch 4: Train Acc = 0.7895, Val Acc = 0.7873\n",
      "Epoch 5: Train Acc = 0.8075, Val Acc = 0.7956\n",
      "Epoch 6: Train Acc = 0.8192, Val Acc = 0.7973\n",
      "Epoch 7: Train Acc = 0.8175, Val Acc = 0.8107\n",
      "Epoch 8: Train Acc = 0.8309, Val Acc = 0.8107\n",
      "Epoch 9: Train Acc = 0.8317, Val Acc = 0.8191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 Epochs:  20%|██        | 14/70 [00:00<00:00, 126.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc = 0.8363, Val Acc = 0.8258\n",
      "Epoch 11: Train Acc = 0.8368, Val Acc = 0.8308\n",
      "Epoch 12: Train Acc = 0.8497, Val Acc = 0.8375\n",
      "Epoch 13: Train Acc = 0.8535, Val Acc = 0.8492\n",
      "Epoch 14: Train Acc = 0.8652, Val Acc = 0.8492\n",
      "Epoch 15: Train Acc = 0.8698, Val Acc = 0.8476\n",
      "Epoch 16: Train Acc = 0.8790, Val Acc = 0.8559\n",
      "Epoch 17: Train Acc = 0.8794, Val Acc = 0.8576\n",
      "Epoch 18: Train Acc = 0.8861, Val Acc = 0.8559\n",
      "Epoch 19: Train Acc = 0.8933, Val Acc = 0.8610\n",
      "Epoch 20: Train Acc = 0.9021, Val Acc = 0.8610\n",
      "Epoch 21: Train Acc = 0.8995, Val Acc = 0.8643\n",
      "Epoch 22: Train Acc = 0.9041, Val Acc = 0.8677\n",
      "Epoch 23: Train Acc = 0.9087, Val Acc = 0.8744\n",
      "Epoch 24: Train Acc = 0.9100, Val Acc = 0.8727\n",
      "Epoch 25: Train Acc = 0.9142, Val Acc = 0.8760\n",
      "Epoch 26: Train Acc = 0.9221, Val Acc = 0.8811\n",
      "Epoch 27: Train Acc = 0.9276, Val Acc = 0.8811\n",
      "Epoch 28: Train Acc = 0.9242, Val Acc = 0.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5 Epochs:  50%|█████     | 35/70 [00:00<00:00, 131.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Acc = 0.9309, Val Acc = 0.8777\n",
      "Epoch 30: Train Acc = 0.9334, Val Acc = 0.8777\n",
      "Epoch 31: Train Acc = 0.9393, Val Acc = 0.8777\n",
      "Epoch 32: Train Acc = 0.9401, Val Acc = 0.8811\n",
      "Epoch 33: Train Acc = 0.9485, Val Acc = 0.8811\n",
      "Epoch 34: Train Acc = 0.9519, Val Acc = 0.8794\n",
      "Epoch 35: Train Acc = 0.9494, Val Acc = 0.8811\n",
      "Early stopping at epoch 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fold accuracy plot: plots_MLP2_593/mlp_accuracy_fold_5.png\n",
      "\n",
      "Fold 5 MLP Best Validation Accuracy: 0.8811\n",
      "MLP Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.88       314\n",
      "           1       0.86      0.90      0.88       283\n",
      "\n",
      "    accuracy                           0.88       597\n",
      "   macro avg       0.88      0.88      0.88       597\n",
      "weighted avg       0.88      0.88      0.88       597\n",
      "\n",
      "MLP Confusion Matrix:\n",
      "[[271  43]\n",
      " [ 28 255]]\n",
      "\n",
      "✅ Saved combined validation accuracy plot: plots_MLP2_593/mlp_combined_validation_accuracy.png\n",
      "\n",
      "Final Best Accuracy of MLP (across all folds): 0.8945\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('PLOTS_MLP_2', exist_ok=True)\n",
    "\n",
    "# Load and standardize data\n",
    "X_resampled = np.load('X_resampled.npy')\n",
    "y_resampled = np.load('y_resampled.npy')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_resampled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "X = torch.tensor(X_resampled, dtype=torch.float32)\n",
    "y = torch.tensor(y_resampled, dtype=torch.long)\n",
    "\n",
    "# Define MLP model with dropout\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden1=128, hidden2=64, dropout_rate=0.3):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.out = nn.Linear(hidden2, 2)  # binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        return self.out(x)\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "patience = 20\n",
    "epochs = 70\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4  # like alpha in sklearn\n",
    "\n",
    "all_val_acc_per_fold = []  # for combined plot\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n=== Fold {fold +1} ===\")\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    model = MLPNet(input_dim=X.shape[1])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    no_improve_count = 0\n",
    "    train_acc_per_epoch = []\n",
    "    val_acc_per_epoch = []\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs +1), desc=f'Fold {fold +1} Epochs'):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Train accuracy\n",
    "        _, preds_train = torch.max(outputs, 1)\n",
    "        train_acc = accuracy_score(y_train.numpy(), preds_train.numpy())\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            _, preds_val = torch.max(val_outputs, 1)\n",
    "            val_acc = accuracy_score(y_val.numpy(), preds_val.numpy())\n",
    "\n",
    "        train_acc_per_epoch.append(train_acc)\n",
    "        val_acc_per_epoch.append(val_acc)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_preds = preds_val.numpy()\n",
    "            no_improve_count = 0\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "\n",
    "        if no_improve_count >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Acc = {train_acc:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "\n",
    "    fold_accuracies.append(best_val_acc)\n",
    "    all_val_acc_per_fold.append(val_acc_per_epoch)  # store validation accuracy per fold for combined plot\n",
    "\n",
    "    # ✅ Save per-fold accuracy plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, len(train_acc_per_epoch)+1), train_acc_per_epoch, label='Train Accuracy')\n",
    "    plt.plot(range(1, len(val_acc_per_epoch)+1), val_acc_per_epoch, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0.7, 1)\n",
    "    plt.title(f'Fold {fold +1} MLP Accuracy Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    fold_plot_filename = f'PLOTS_MLP_2/mlp_accuracy_fold_{fold +1}.png'\n",
    "    plt.savefig(fold_plot_filename)\n",
    "    plt.close()\n",
    "    print(f\"✅ Saved fold accuracy plot: {fold_plot_filename}\")\n",
    "\n",
    "    # Report\n",
    "    print(f\"\\nFold {fold +1} MLP Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    print(\"MLP Classification Report:\")\n",
    "    print(classification_report(y_val.numpy(), best_val_preds))\n",
    "    print(\"MLP Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val.numpy(), best_val_preds))\n",
    "\n",
    "# ✅ Plot combined validation accuracy curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "for fold_idx, val_accs in enumerate(all_val_acc_per_fold):\n",
    "    plt.plot(range(1, len(val_accs)+1), val_accs, label=f'Fold {fold_idx +1}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.ylim(0.7, 1)\n",
    "plt.title('Validation Accuracy Over Epochs (All Folds)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "combined_plot_filename = 'PLOTS_MLP_2/mlp_combined_validation_accuracy.png'\n",
    "plt.savefig(combined_plot_filename)\n",
    "plt.close()\n",
    "print(f\"\\n✅ Saved combined validation accuracy plot: {combined_plot_filename}\")\n",
    "\n",
    "# Final summary\n",
    "final_best_accuracy = max(fold_accuracies)\n",
    "print(f\"\\nFinal Best Accuracy of MLP (across all folds): {final_best_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kinzaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
